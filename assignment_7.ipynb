{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "assignment_7",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cb51470fb2d4252822adcb862ad1ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_324f2f380e6a46449da87a7cca5dca2e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9dab3d6282494784bc7f58340807beab",
              "IPY_MODEL_90d7b71422fd489a85d744c560702e9d"
            ]
          }
        },
        "324f2f380e6a46449da87a7cca5dca2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9dab3d6282494784bc7f58340807beab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_89161162066c4eccafae80dd13067ad7",
            "_dom_classes": [],
            "description": "epoch 1",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1464,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1464,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a3d018ec019402b94be0afc1bc01ccd"
          }
        },
        "90d7b71422fd489a85d744c560702e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_33afd20fa0dc45beb264170ed988dcd6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 1464/1464 [05:59&lt;00:00,  4.02it/s, loss=7.07347]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_002e89cbf49540138f9870516ec00a47"
          }
        },
        "89161162066c4eccafae80dd13067ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a3d018ec019402b94be0afc1bc01ccd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33afd20fa0dc45beb264170ed988dcd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "002e89cbf49540138f9870516ec00a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f46ea1f0e3124980b82d04d6c56e9af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7dee2d80754a4c5287cf9ef05f55800e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ad08b35ec9fe462e8ccaae14dfd0efa1",
              "IPY_MODEL_eefe6e5db66942d9b651ddfd046a52a3"
            ]
          }
        },
        "7dee2d80754a4c5287cf9ef05f55800e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad08b35ec9fe462e8ccaae14dfd0efa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_846a450bc31d4cbbaf32f502e8a0f6ed",
            "_dom_classes": [],
            "description": "eval epoch 1",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 366,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 366,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b66a8206a2814bbb911800278b900370"
          }
        },
        "eefe6e5db66942d9b651ddfd046a52a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28870a66547e43379bb3a6c44b969938",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 366/366 [00:30&lt;00:00, 12.10it/s, loss=6.55695]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22aceb1eafa049a29991de8b19ce45d5"
          }
        },
        "846a450bc31d4cbbaf32f502e8a0f6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b66a8206a2814bbb911800278b900370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28870a66547e43379bb3a6c44b969938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22aceb1eafa049a29991de8b19ce45d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e1b92caa4db46eba12d151a03bacabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ced5615562f14783a3a73defcd74830f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f3c04c496b54e7ca35b98e65e9b606e",
              "IPY_MODEL_3710718d0e554b5d84a39250bd0d1989"
            ]
          }
        },
        "ced5615562f14783a3a73defcd74830f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f3c04c496b54e7ca35b98e65e9b606e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8352cbffc3c54f69909a95788f202d12",
            "_dom_classes": [],
            "description": "epoch 2",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 1464,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 179,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29b64a32a02b41a9b60f6ee7d3aa0273"
          }
        },
        "3710718d0e554b5d84a39250bd0d1989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c654cc4adfe45c3bfe55cdfb5bcea9d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 12% 179/1464 [00:44&lt;05:15,  4.08it/s, loss=6.48321]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ade4e9666c44e439d457e3e0276931c"
          }
        },
        "8352cbffc3c54f69909a95788f202d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29b64a32a02b41a9b60f6ee7d3aa0273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c654cc4adfe45c3bfe55cdfb5bcea9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ade4e9666c44e439d457e3e0276931c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flying-bear/kompluxternaya/blob/master/assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOhJI37CJnsc",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 7\n",
        "\n",
        "Train a Transformer model for Machine Translation from Russian to English.  \n",
        "Dataset: http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz   \n",
        "Make all source and target text to lower case.  \n",
        "\n",
        "Score: corpus-bleu `nltk.translate.bleu_score.corpus_bleu`  \n",
        "Use last 1000 sentences for model evalutation (test dataset).  \n",
        "Use your target sequence tokenization for BLEU score.  \n",
        "Use max_len=50 for sequence prediction.  \n",
        "\n",
        "\n",
        "Hint: You may consider much smaller model, than shown in the example.  \n",
        "\n",
        "Baselines:  \n",
        "[4 point] BLEU = 0.05  \n",
        "[6 point] BLEU = 0.10  \n",
        "[9 point] BLEU = 0.15  \n",
        "\n",
        "[1 point] Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
        "\n",
        "\n",
        "Readings:\n",
        "1. BLUE score how to https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "1. Transformer code and comments http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-YVev80JuzS",
        "colab_type": "code",
        "outputId": "2bb1c016-222f-40e9-ad57-762c9bedbc4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!wget http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-16 16:46:02--  http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113157482 (108M) [application/x-gzip]\n",
            "Saving to: ‘training-parallel-nc-v13.tgz.4’\n",
            "\n",
            "training-parallel-n 100%[===================>] 107.92M  96.3MB/s    in 1.1s    \n",
            "\n",
            "2020-02-16 16:46:03 (96.3 MB/s) - ‘training-parallel-nc-v13.tgz.4’ saved [113157482/113157482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5OPsBlKJ4Mq",
        "colab_type": "code",
        "outputId": "243bd61d-d51d-4a6c-c265-35ee3ec13b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!gunzip -c training-parallel-nc-v13.tgz | tar xvf - "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training-parallel-nc-v13/\n",
            "training-parallel-nc-v13/news-commentary-v13.ru-en.ru\n",
            "training-parallel-nc-v13/news-commentary-v13.cs-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.de-en.de\n",
            "training-parallel-nc-v13/news-commentary-v13.ru-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.zh-en.zh\n",
            "training-parallel-nc-v13/news-commentary-v13.zh-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.cs-en.cs\n",
            "training-parallel-nc-v13/news-commentary-v13.de-en.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ccf8cedc-f7f3-46a4-fe33-2fc0b8a7d229",
        "id": "vIToU2TqXqaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!rm training-parallel-nc-v13/news-commentary-v13.cs-en.en\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.de-en.de\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.zh-en.zh\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.zh-en.en\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.cs-en.cs\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.de-en.en\n",
        "\n",
        "!ls training-parallel-nc-v13"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "news-commentary-v13.ru-en.en\t   news-commentary-v13-test.ru-en.ru\n",
            "news-commentary-v13.ru-en.ru\t   news-commentary-v13-train.ru-en.en\n",
            "news-commentary-v13-test.ru-en.en  news-commentary-v13-train.ru-en.ru\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQqHFHGaK2wg",
        "colab_type": "code",
        "outputId": "02677a6e-d3a0-4f4a-e39c-03335455417f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSfZTW5vJnsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy \n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import sentencepiece as spm\n",
        "import seaborn\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu as bleu\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Field\n",
        "from torchtext.datasets import TranslationDataset\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "\n",
        "\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHUGNqqmSF8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "max_len=50\n",
        "max_src_in_batch = 25000\n",
        "max_tgt_in_batch = 25000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51VO9rWafLle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else tt.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h48RAm9MIRQ",
        "colab_type": "text"
      },
      "source": [
        "## TOKENIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx9JbkQdO-Vq",
        "colab_type": "text"
      },
      "source": [
        "ENGLISH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6E-1psJMV7H",
        "colab_type": "code",
        "outputId": "a2318313-2b7f-449c-91fb-91ca3fcc76ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spm.SentencePieceTrainer.Train('--input=training-parallel-nc-v13/news-commentary-v13.ru-en.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HR3MhUlMaF2",
        "colab_type": "code",
        "outputId": "16f68f12-416d-4075-bf3a-18c8874d2cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qxwL0L3MKnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT = Field(\n",
        "    fix_length=max_len,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqv-rfdPAMm",
        "colab_type": "text"
      },
      "source": [
        "RUSSIAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ac3880ac-1d40-4266-a209-684c16cff183",
        "id": "SQREmq5dOy-Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spm.SentencePieceTrainer.Train('--input=training-parallel-nc-v13/news-commentary-v13.ru-en.ru --model_prefix=bpe_ru --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d93584d7-c162-4812-a992-cf6d4b703991",
        "id": "W8EL_AUCOy-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tok_ru = spm.SentencePieceProcessor()\n",
        "tok_ru.load('bpe_ru.model')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2etE_sa4Oy-r",
        "colab": {}
      },
      "source": [
        "TEXT = Field(\n",
        "    fix_length=max_len,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdJPhkqRVQYd",
        "colab_type": "text"
      },
      "source": [
        "## DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfAXuVlaYCrP",
        "colab_type": "text"
      },
      "source": [
        "NB! every sentence is a new line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ugalZrSkoi",
        "colab_type": "text"
      },
      "source": [
        "separate last 1000 sentences for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGU1WHbuPNmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('training-parallel-nc-v13/news-commentary-v13.ru-en.ru', 'r', encoding='utf-8') as f:\n",
        "  source_text = f.readlines()\n",
        "\n",
        "with open('training-parallel-nc-v13/news-commentary-v13-test.ru-en.ru', 'w', encoding='utf-8') as f:\n",
        "  f.write(''.join(source_text[:1000]))\n",
        "\n",
        "with open('training-parallel-nc-v13/news-commentary-v13-train.ru-en.ru', 'w', encoding='utf-8') as f:\n",
        "  f.write(''.join(source_text[1000:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC5Heye6QwaX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc1a87fc-0e47-41ab-eaba-d7b77a8611f6"
      },
      "source": [
        "with open('training-parallel-nc-v13/news-commentary-v13-train.ru-en.ru', 'r', encoding='utf-8') as f:\n",
        "  len_source = len(f.readlines())\n",
        "print(len_source)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "234159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ88_1P7Q_9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('training-parallel-nc-v13/news-commentary-v13.ru-en.en', 'r', encoding='utf-8') as f:\n",
        "  target_text = f.readlines()\n",
        "\n",
        "with open('training-parallel-nc-v13/news-commentary-v13-test.ru-en.en', 'w', encoding='utf-8') as f:\n",
        "  f.write(''.join(target_text[:1000]))\n",
        "\n",
        "with open('training-parallel-nc-v13/news-commentary-v13-train.ru-en.en', 'w', encoding='utf-8') as f:\n",
        "  f.write(''.join(target_text[1000:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE6QNoecYCQL",
        "colab_type": "code",
        "outputId": "216d2816-23b5-4e69-8460-8b0cf1cfb6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open('training-parallel-nc-v13/news-commentary-v13-train.ru-en.en', 'r', encoding='utf-8') as f:\n",
        "  len_target = len(f.readlines())\n",
        "print(len_target)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "234159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOOts08wVSe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = TranslationDataset('training-parallel-nc-v13/news-commentary-v13-train', exts=('.ru-en.ru', '.ru-en.en'), fields=(TEXT, TGT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O25Mg2vtWSzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn, vld = dataset.split(split_ratio=0.8, stratified=False, random_state=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6c_5GYJRdIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tst = TranslationDataset('training-parallel-nc-v13/news-commentary-v13-test', exts=('.ru-en.ru', '.ru-en.en'), fields=(TEXT, TGT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "roX11K00Oy-w",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(trn.src, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdQ5WgZgMT5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT.build_vocab(trn.trg, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVZsoGTq1lZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = TEXT.vocab.stoi[\"<pad>\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7c_4_xe2HmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_tgt_size = len(TGT.vocab)\n",
        "vocab_src_size = len(TEXT.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO9sqXPgL_5E",
        "colab_type": "text"
      },
      "source": [
        "## MODEL ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eio-sP-pxw2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0ykF_8ryi8o",
        "colab_type": "text"
      },
      "source": [
        "### ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26HrBNXJyGxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMcprSb1yKfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLi814xgyO8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0-xlCzOyQ6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K80bjWp0yVX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCdisr04yd-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL4fviG_ylsi",
        "colab_type": "text"
      },
      "source": [
        "### DECODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS9P7xlUyn3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIrsHMWkyrVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv6M2a12yttx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuXqm-9-yxKh",
        "colab_type": "text"
      },
      "source": [
        "### ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoUmOS5oyy24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RhRrItEy1gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2dJxUidy7LV",
        "colab_type": "text"
      },
      "source": [
        "### POSITION WISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXielEJoy47w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ziV0PZy-8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1viqy56zBHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wUjvefdzGsQ",
        "colab_type": "text"
      },
      "source": [
        "### ALL TOGETHER NOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQB2IvnyzEfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai41imjHzVCu",
        "colab_type": "text"
      },
      "source": [
        "## BATCHES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPDr1PXPzZ3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EUsClDlbQ5_",
        "colab_type": "text"
      },
      "source": [
        "## ITERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H8EMt2hautk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    return Batch(src, trg, pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7L3OuP6eBBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBWaTkVMcm9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_itr = MyIterator(trn, batch_size=BATCH_SIZE, device=device,\n",
        "                     repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                     batch_size_fn=None, train=True)\n",
        "vld_itr = MyIterator(vld, batch_size=BATCH_SIZE, device=device,\n",
        "                     repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                     batch_size_fn=None, train=False)\n",
        "tst_itr = MyIterator(tst, batch_size=BATCH_SIZE, device=device,\n",
        "                     repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                     batch_size_fn=None, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROF1cKuMWlQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ac7e200d-29a9-4ee7-84c9-d4a4c66e9ce4"
      },
      "source": [
        "print(len(trn))\n",
        "print(len(list(iter(trn_itr))))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "187327\n",
            "1464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3qQe70_W_sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "392decf4-3489-48f1-fdee-66799a5e51a7"
      },
      "source": [
        "print(len(vld))\n",
        "print(len(list(iter(vld_itr))))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46832\n",
            "366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUYs2_fCfdMW",
        "colab_type": "code",
        "outputId": "9848fbe1-d846-4386-f836-dc882c4b38c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "example = next(iter(trn_itr))\n",
        "example"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 128]\n",
              "\t[.src]:[torch.cuda.LongTensor of size 128x50 (GPU 0)]\n",
              "\t[.trg]:[torch.cuda.LongTensor of size 128x50 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tNrNZs3z95G",
        "colab_type": "code",
        "outputId": "1829e984-5f69-4edf-d939-433cf35afa62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "example.src"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   2,  156,   13,  ...,    1,    1,    1],\n",
              "        [   2, 2733, 8947,  ...,    1,    1,    1],\n",
              "        [   2,    5,   25,  ...,    1,    1,    1],\n",
              "        ...,\n",
              "        [   2,    5,   19,  ...,    1,    1,    1],\n",
              "        [   2,    5,   19,  ...,    1,    1,    1],\n",
              "        [   2, 2545,  128,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rrqsKSNAaeW",
        "colab_type": "text"
      },
      "source": [
        "### TEXT HANDLER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyNnxT1OgmwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_text(seq, field=TEXT):\n",
        "  \"\"\"gets a list of tokens given a list of items using a vocabulary of a given field\n",
        "  \n",
        "  :param seq: list of int, token ids\n",
        "  :param field: trained torchtext.data.Field, optional, default TEXT\n",
        "  :return: list of str, tokens\n",
        "  \"\"\"\n",
        "  return [field.vocab.itos[v] for v in seq]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaOtzBR1g1DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_tok(tok_list, eos_token='</s>', sos_token='</s>', pad_token='<pad>'): \n",
        "  \"\"\"\n",
        "  removes paading and joins subword tokens\n",
        "\n",
        "  :param tok_list: list of str, subword tokens with \"_\" indicating a word beginning (space)\n",
        "  :param eos_token: str, end of sequence token, optional, default </s> \n",
        "  :param sos_token: str, start of sequence token, optional, default <s> \n",
        "  :param pad_token: str, pad token, optional, default <pad> \n",
        "  :return: str, joined sentence\n",
        "  \"\"\"\n",
        "  while tok_list[-1] == pad_token:\n",
        "    tok_list = tok_list[:-1]\n",
        "  if tok_list[-1] == eos_token:\n",
        "    return ''.join(tok_list[:-1]).replace('▁', ' ').strip() + ' ' + eos_token\n",
        "  else:\n",
        "    return ''.join(tok_list).replace('▁', ' ').strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPWm7xcVgAN0",
        "colab_type": "code",
        "outputId": "ab5d794b-2264-4906-a018-1601693a7a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "join_tok(to_text(example.src[0], TEXT))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> кто-то может предложить ликвидацию налоговых льгот, чтобы сделать снижение ставки нейтральным для доходов бюджета. </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_WjGHF42HKy",
        "colab_type": "code",
        "outputId": "6a67e724-2e54-4d02-88ab-08f3cab67bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "join_tok(to_text(example.trg[0], TGT))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> one can imagine closing loopholes to render rate cuts revenue neutral. but one person’s loophole is another’s entitlement. </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjMQN8F07Z-",
        "colab_type": "text"
      },
      "source": [
        "## LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1265QlK1HSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm, training=True):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.reshape(-1, x.size(-1)), y.reshape(-1)) / norm\n",
        "        if training:\n",
        "          loss.backward()\n",
        "          if self.opt is not None:\n",
        "              self.opt.step()\n",
        "              self.opt.zero_grad()\n",
        "        return loss.detach().item() * norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiMfZ4pnjYbI",
        "colab_type": "text"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P-_61RvzpQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute, curr_epoch, n_batches, training=True):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    counter = 0\n",
        "    running_loss = 0\n",
        "    if training:\n",
        "      iterator = tqdm_notebook(data_iter, total=n_batches, desc=f'epoch {curr_epoch + 1}', leave=True)\n",
        "    else:\n",
        "      iterator = tqdm_notebook(data_iter, total=n_batches, desc=f'eval epoch {curr_epoch + 1}', leave=True)\n",
        "    train_losses = []\n",
        "    for i, batch in enumerate(iterator):\n",
        "        if batch.ntokens != 0:\n",
        "          out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
        "          loss = loss_compute(out, batch.trg_y, batch.ntokens, training=training)\n",
        "          total_loss += loss\n",
        "          total_tokens += batch.ntokens\n",
        "          tokens += batch.ntokens\n",
        "          train_losses.append(loss.item())\n",
        "\n",
        "          running_loss = total_loss / total_tokens\n",
        "          iterator.set_postfix(loss=f'{running_loss:.5f}')\n",
        "\n",
        "          # if i % 100 == 1:\n",
        "          #     elapsed = time.time() - start\n",
        "          #     print(f\"Epoch Step: {i} Loss on this batch: {loss} Time Elapsed {elapsed}\") ## Tokens per Sec: {tokens / elapsed}\n",
        "          #     # start = time.time()\n",
        "          #     # tokens = 0\n",
        "        else:\n",
        "          counter += 1\n",
        "          if counter % 100 == 0:\n",
        "            print('100 zero-size batches')\n",
        "    return running_loss, train_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt2Ebhpu0in5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = make_model(vocab_src_size, vocab_tgt_size, N=2)\n",
        "model.cuda()\n",
        "# criterion = LabelSmoothing(vocab_tgt_size, padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, reduction='sum')\n",
        "criterion.cuda()\n",
        "model_opt = torch.optim.Adam(model.parameters(), lr=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoP0UJ4N360i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e1d6f64-fc88-4c96-8d9f-f03b3f4a634e"
      },
      "source": [
        "n_batches = len(list(iter(trn_itr)))\n",
        "n_batches"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSBK8wrFZOph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31132341-4444-4e80-c138-b68810296dd2"
      },
      "source": [
        "n_batches_vld = len(list(iter(vld_itr)))\n",
        "n_batches_vld"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "366"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yTC2i_JLkXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "4cb51470fb2d4252822adcb862ad1ce9",
            "324f2f380e6a46449da87a7cca5dca2e",
            "9dab3d6282494784bc7f58340807beab",
            "90d7b71422fd489a85d744c560702e9d",
            "89161162066c4eccafae80dd13067ad7",
            "0a3d018ec019402b94be0afc1bc01ccd",
            "33afd20fa0dc45beb264170ed988dcd6",
            "002e89cbf49540138f9870516ec00a47",
            "f46ea1f0e3124980b82d04d6c56e9af7",
            "7dee2d80754a4c5287cf9ef05f55800e",
            "ad08b35ec9fe462e8ccaae14dfd0efa1",
            "eefe6e5db66942d9b651ddfd046a52a3",
            "846a450bc31d4cbbaf32f502e8a0f6ed",
            "b66a8206a2814bbb911800278b900370",
            "28870a66547e43379bb3a6c44b969938",
            "22aceb1eafa049a29991de8b19ce45d5",
            "8e1b92caa4db46eba12d151a03bacabb",
            "ced5615562f14783a3a73defcd74830f",
            "2f3c04c496b54e7ca35b98e65e9b606e",
            "3710718d0e554b5d84a39250bd0d1989",
            "8352cbffc3c54f69909a95788f202d12",
            "29b64a32a02b41a9b60f6ee7d3aa0273",
            "4c654cc4adfe45c3bfe55cdfb5bcea9d",
            "2ade4e9666c44e439d457e3e0276931c"
          ]
        },
        "outputId": "9bd95d71-0a07-4882-bf99-2338559d3488"
      },
      "source": [
        "train_history = []\n",
        "eval_history = []\n",
        "per_epoch_train_history = []\n",
        "per_epoch_eval_history = []\n",
        "for epoch in range(5):\n",
        "        model.train()\n",
        "        train_total_loss, train_losses = run_epoch((rebatch(pad_idx, b) for b in trn_itr), model, SimpleLossCompute(model.generator, criterion, opt=model_opt), epoch, n_batches)\n",
        "        train_history += train_losses\n",
        "        per_epoch_train_history.append(train_total_loss)\n",
        "        with torch.no_grad():\n",
        "          model.eval()\n",
        "          eval_total_loss, eval_losses = run_epoch((rebatch(pad_idx, b) for b in vld_itr), model, SimpleLossCompute(model.generator, criterion, opt=None), epoch, n_batches_vld, training=False)\n",
        "          eval_history += eval_losses\n",
        "          per_epoch_eval_history.append(eval_total_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cb51470fb2d4252822adcb862ad1ce9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='epoch 1', max=1464, style=ProgressStyle(description_width='in…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f46ea1f0e3124980b82d04d6c56e9af7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='eval epoch 1', max=366, style=ProgressStyle(description_width…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e1b92caa4db46eba12d151a03bacabb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='epoch 2', max=1464, style=ProgressStyle(description_width='in…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJZFWsMuUHQv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8137badc-0f96-4124-8722-77e7d395e043"
      },
      "source": [
        "print(len(train_history))\n",
        "print(len(eval_history))\n",
        "print(len(per_epoch_train_history))\n",
        "print(len(per_epoch_eval_history))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2928\n",
            "732\n",
            "2\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzXwjirY_o3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBTwv66EzHnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "plt.plot(range(int(len(train_history)/10)), [el.item() for el in train_history[::10]], np.array(range(int(len(eval_history)/10)))*4, [el.item() for el in eval_history[::10]])\n",
        "plt.legend(('train loss', 'valid loss by batch'),\n",
        "           loc='center', prop={'size': 18})\n",
        "plt.title('Training process', fontsize=20)\n",
        "plt.xlabel('Iterations', fontsize=16)\n",
        "plt.ylabel('Loss function', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmXF9I9ZS7Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smooth = lambda y: gaussian_filter1d(y, sigma=100)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.plot(range(len(train_history)), smooth([el.item() for el in train_history]), np.array(range(len(eval_history)))*4, smooth([el.item() for el in eval_history]))\n",
        "plt.legend(('train loss', 'valid loss by batch'),\n",
        "           loc='center', prop={'size': 18})\n",
        "plt.title('Smoothed training process', fontsize=20)\n",
        "plt.xlabel('Iterations', fontsize=16)\n",
        "plt.ylabel('Loss function (smoothed)', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE6wpimPasuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icYGV9C0bBs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_loaded = make_model(vocab_src_size, vocab_tgt_size, N=2)\n",
        "model_loaded.load_state_dict(torch.load('model.pt'))\n",
        "model_loaded.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_3MJXROK5zi",
        "colab_type": "text"
      },
      "source": [
        "## DECODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOOqDLIkK82S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
        "  src_mask = Variable(torch.ones(1, 1, 10) )\n",
        "  print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSOx2etMMPwW",
        "colab_type": "text"
      },
      "source": [
        "ADAPT FROM LAST HW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdTjgIgDMN4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search_from_start(model, k=3, max_len=30, eos='<eos>'):\n",
        "  \"\"\"\n",
        "  autoregressive language modeling beam search decoder\n",
        "\n",
        "  :param model: trained torch model, predicts logits torch tensor (of shape (batch, vocab, seq)) form batch tensor (of shape (batch, seq))\n",
        "  :param k: int, beam width, optional, default 3\n",
        "  :param max_len: int, maximal number of words produced, optional, default 30\n",
        "  :param eos: str, end of sentence token, optional, default '<eos>'\n",
        "  :return: list of str (of len k), top predicted sequences\n",
        "  \"\"\"\n",
        "  id_eos = token_to_id(eos)\n",
        "  with tt.no_grad():\n",
        "    pred = model(tt.tensor([[start]]))[:,:,-1][0]\n",
        "    toks = tt.argsort(pred, descending=True)[:k].tolist()\n",
        "    tok_probs = pred[toks].tolist()\n",
        "    candidates = [([start, toks[i]], tok_probs[i]) for i in range(len(toks))]\n",
        "    while max_len > max([len(seq) for seq, prob in candidates]):\n",
        "      # expand each current candidate that doesn't have an EOS at the end\n",
        "      new_candidates = []\n",
        "      for seq, prob in candidates:\n",
        "        if seq[-1] != id_eos:\n",
        "          pred = model(tt.tensor([seq]))[:,:,-1][0]\n",
        "          toks = tt.argsort(pred, descending=True)[:k].tolist()\n",
        "          # probs = np.log(soft_with_temp(pred))\n",
        "          tok_probs = pred[toks].tolist()\n",
        "          new_candidates += [(seq[:]+[toks[i]], prob+tok_probs[i]) for i in range(len(toks))]\n",
        "        else:\n",
        "          new_candidates += [(seq, prob)]\n",
        "      candidates = sorted(new_candidates, key=lambda tup:tup[1])[:k] # order all candidates by score and select k best\n",
        "  return [join_tok(to_text(seq)) for seq, prob in candidates]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}