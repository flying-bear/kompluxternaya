{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "new_assignment_9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flying-bear/kompluxternaya/blob/master/new_assignment_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuMyevR2w0Fa",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 9\n",
        "\n",
        "Use data from `https://github.com/thedenaas/hse_seminars/tree/master/2018/seminar_13/data.zip`  \n",
        "Implement model in pytorch from [\"An Unsupervised Neural Attention Model for Aspect Extraction, He et al, 2017\"](https://www.comp.nus.edu.sg/~leews/publications/acl17.pdf), also desribed in seminar notes.  \n",
        "\n",
        "\n",
        "You can use sentence embeddings with attention **[7 points]**:  \n",
        "$z_s = \\sum_{i}^n \\alpha_i e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$\\alpha_i = softmax(d_i)$  attention weight for i-th token  \n",
        "$d_i = e_{w_i}^T M y_s$ attention with trainable matrix $M \\in R^{dxd}$  \n",
        "$y_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, y_s \\in R^d$ sentence context  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        "\n",
        "**Or** just use sentence embedding as an average over word embeddings **[5 points]**:  \n",
        "$z_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        " \n",
        "$p_t = softmax(W z_s + b), p_t \\in R^K$ topic weights for sentence $s$, with trainable matrix $W \\in R^{dxK}$ and bias vector $b \\in R^K$  \n",
        "$r_s = T^T p_t, r_s \\in R^d$ reconstructed sentence embedding as a weighted sum of topic embeddings   \n",
        "$T \\in R^{Kxd}$ trainable matrix of topic embeddings, K=number of topics\n",
        "\n",
        "\n",
        "**Training objective**:\n",
        "$$ J = \\sum_{s \\in D} \\sum_{i=1}^m max(0, 1-r_s^T z_s + r_s^T n_i) + \\lambda ||T^T T - I ||^2_F  $$\n",
        "where   \n",
        "$m$ random sentences are sampled as negative examples from dataset $D$ for each sentence $s$  \n",
        "$n_i = \\frac 1 n \\sum_{i=j}^n e_{w_j}$ average of word embeddings in the i-th sentence  \n",
        "$||T^T T - I ||_F$ regularizer, that enforces matrix $T$ to be orthogonal  \n",
        "$||A||^2_F = \\sum_{i=1}^N\\sum_{j=1}^M a_{ij}^2, A \\in R^{NxM}$ Frobenius norm\n",
        "\n",
        "\n",
        "**[3 points]** Compute topic coherence for at least for 3 different number of topics. Use 10 nearest words for each topic. It means you have to train one model for each number of topics. You can use code from seminar notes with word2vec similarity scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7Xhf7Ty0lOd",
        "colab_type": "text"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czt2JCSVw0Fe",
        "colab_type": "code",
        "outputId": "dab27907-d283-4fd4-975b-583c2c4f1d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "!wget -O data.zip https://github.com/thedenaas/hse_seminars/blob/master/2018/seminar_13/data.zip?raw=true "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-20 12:21:32--  https://github.com/thedenaas/hse_seminars/blob/master/2018/seminar_13/data.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/thedenaas/hse_seminars/raw/master/2018/seminar_13/data.zip [following]\n",
            "--2020-03-20 12:21:33--  https://github.com/thedenaas/hse_seminars/raw/master/2018/seminar_13/data.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2018/seminar_13/data.zip [following]\n",
            "--2020-03-20 12:21:34--  https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2018/seminar_13/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9927168 (9.5M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   9.47M  18.9MB/s    in 0.5s    \n",
            "\n",
            "2020-03-20 12:21:36 (18.9 MB/s) - ‘data.zip’ saved [9927168/9927168]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVc2yVTixVTG",
        "colab_type": "code",
        "outputId": "5c4adff2-732e-4032-dacb-644f94211c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "  inflating: data.txt                \n",
            "  inflating: stopwords.txt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNs1xH391CKM",
        "colab_type": "text"
      },
      "source": [
        "## video memeory?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puoth8LzC36v",
        "colab_type": "code",
        "outputId": "efa12aa5-63cb-43b1-cb85-be916efc0b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Mar 20 12:21:43 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbM-mkh1I-i",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRABeksSAco0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchtext.data import Field, TabularDataset, Iterator\n",
        "\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vocSo75u0i0S",
        "colab_type": "code",
        "outputId": "a233df2a-1e4a-496c-e8f1-d0a4835a8932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA-Afo7HI1PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "random_state = 42\n",
        "num_neg_samples = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5-J9QhXDgi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else tt.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc0HJ1xx1Fqy",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdHPB3x4yhkR",
        "colab_type": "code",
        "outputId": "4a245a62-3b5e-411d-c916-3f8799010a70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('data.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "print(text[:100])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Barclays' defiance of US fines has merit Barclays disgraced itself in many ways during the pre-finan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxQ00GRsyyAv",
        "colab_type": "code",
        "outputId": "814c279f-04bb-4341-da4f-1415583ef051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(text.split('/n'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh_NNHgfxo5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_stop_words = []\n",
        "with open( \"stopwords.txt\", \"r\" ) as f:\n",
        "    for line in f.readlines():\n",
        "        custom_stop_words.append( line.strip().lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkHPlvzD1Vkw",
        "colab_type": "text"
      },
      "source": [
        "### Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI4AFl_N2Xaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_tokenize(text):\n",
        "  return nltk.tokenize.sent_tokenize(text)\n",
        "\n",
        "def tokenize(sent):\n",
        "  return nltk.tokenize.word_tokenize(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlVvZ5Tp-aMI",
        "colab_type": "code",
        "outputId": "55437062-c127-4ce1-8fc5-4ae38957ec0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sent_tokenize('ads kjd.\\n hadkjahd! jhwahjw! a the I')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ads kjd.', 'hadkjahd!', 'jhwahjw!', 'a the I']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Ft7V9QeUhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f992e2e-712d-4468-bcd7-ff249e3e897b"
      },
      "source": [
        "tokenize('jkahdak akwjhadkjhd jhjh hh')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jkahdak', 'akwjhadkjhd', 'jhjh', 'hh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1mKlj4sFdPs",
        "colab_type": "text"
      },
      "source": [
        "### add negative samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxr2-fhJZpxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['pos'] = sent_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NljMpHhpbp2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "62a73e07-cb94-4614-99b8-bc61a96f708d"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>183395</th>\n",
              "      <td>It feels as though Stone realised that some of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183396</th>\n",
              "      <td>There are some fun elements, many involving Rh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183397</th>\n",
              "      <td>I particularly enjoyed a scene in which O’Bria...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183398</th>\n",
              "      <td>His carnivorous snarl fills the immense screen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183399</th>\n",
              "      <td>There’s a playful visual flair to this moment ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      pos\n",
              "183395  It feels as though Stone realised that some of...\n",
              "183396  There are some fun elements, many involving Rh...\n",
              "183397  I particularly enjoyed a scene in which O’Bria...\n",
              "183398  His carnivorous snarl fills the immense screen...\n",
              "183399  There’s a playful visual flair to this moment ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNSOBR6SLgWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_negative(df):\n",
        "  neg_id = np.random.choice(len(df))\n",
        "  return df.iloc[neg_id, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGD371YFcaDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(num_neg_samples):\n",
        "  df[f'neg{i}'] = df['pos'].apply(lambda x: add_negative(df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWURVqRjdPyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b5b9b2d7-bf34-4af1-ef0a-04a50c9ba9a8"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>neg0</th>\n",
              "      <th>neg1</th>\n",
              "      <th>neg2</th>\n",
              "      <th>neg3</th>\n",
              "      <th>neg4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>183395</th>\n",
              "      <td>It feels as though Stone realised that some of...</td>\n",
              "      <td>British fishermen have complained for years th...</td>\n",
              "      <td>I do believe that both Clinton and Trump will ...</td>\n",
              "      <td>It’s hard to see how it can dismantle so many ...</td>\n",
              "      <td>Would the government have been entitled to go ...</td>\n",
              "      <td>She is dressed and positioned in bed like all ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183396</th>\n",
              "      <td>There are some fun elements, many involving Rh...</td>\n",
              "      <td>Who they really are is who they are when they ...</td>\n",
              "      <td>“Sivan and his equally youthful co-writer-prod...</td>\n",
              "      <td>“He’s put his finger on a deeply underapprecia...</td>\n",
              "      <td>“While you’ve got systemic fraud within the EU...</td>\n",
              "      <td>Someone is trying to kill you.’ ‘Why would any...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183397</th>\n",
              "      <td>I particularly enjoyed a scene in which O’Bria...</td>\n",
              "      <td>The reference to Google did not appear to be a...</td>\n",
              "      <td>“There’s Euro elections coming down the track,...</td>\n",
              "      <td>I wanted to be honest and relate to self-consc...</td>\n",
              "      <td>It allows professionals to see, but not change...</td>\n",
              "      <td>Theresa May shows she has every intention of k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183398</th>\n",
              "      <td>His carnivorous snarl fills the immense screen...</td>\n",
              "      <td>“On February 20, we are going to give Jeb Bush...</td>\n",
              "      <td>For a change, it’s worth acknowledging when we...</td>\n",
              "      <td>Rihanna – Drunk On Love Rihanna covered Tame I...</td>\n",
              "      <td>It happened because everything about Work clic...</td>\n",
              "      <td>Hilary Benn, the shadow foreign secretary, sai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183399</th>\n",
              "      <td>There’s a playful visual flair to this moment ...</td>\n",
              "      <td>During the referendum campaign, concerns were ...</td>\n",
              "      <td>If the litter bothers you, at least you can so...</td>\n",
              "      <td>Felice Bauer, a cousin of the Brod family who ...</td>\n",
              "      <td>The former Manchester City player is so much m...</td>\n",
              "      <td>He starts talking about the “Icelandic music s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      pos  ...                                               neg4\n",
              "183395  It feels as though Stone realised that some of...  ...  She is dressed and positioned in bed like all ...\n",
              "183396  There are some fun elements, many involving Rh...  ...  Someone is trying to kill you.’ ‘Why would any...\n",
              "183397  I particularly enjoyed a scene in which O’Bria...  ...  Theresa May shows she has every intention of k...\n",
              "183398  His carnivorous snarl fills the immense screen...  ...  Hilary Benn, the shadow foreign secretary, sai...\n",
              "183399  There’s a playful visual flair to this moment ...  ...  He starts talking about the “Icelandic music s...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olbrxIToduax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('text.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h0OTeke1Nd1",
        "colab_type": "text"
      },
      "source": [
        "### Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAKc5GNiYVbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = Field(include_lengths=False, \n",
        "             batch_first=True, \n",
        "             tokenize=tokenize,\n",
        "             lower=True,\n",
        "             stop_words=custom_stop_words)\n",
        "\n",
        "datafields = [('pos',TEXT), *[(f'neg{i}', TEXT) for i in range(num_neg_samples)]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnHbWGJBgbcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn = TabularDataset(path=\"text.csv\",\n",
        "                     format='csv',\n",
        "                     skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "                     fields=datafields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lSBb1kfWoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(trn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NuFSL9TfxEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(TEXT.vocab.itos) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4LaUloTQenp",
        "colab_type": "text"
      },
      "source": [
        "### Iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opILml5cgINj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_itr = Iterator(trn, batch_size, device=DEVICE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXbsm9ptjGUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "879bfccd-5f5d-41f0-f803-b189fef74051"
      },
      "source": [
        "example_batch = next(iter(trn_itr))\n",
        "example_batch"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 256]\n",
              "\t[.pos]:[torch.cuda.LongTensor of size 256x68 (GPU 0)]\n",
              "\t[.neg0]:[torch.cuda.LongTensor of size 256x56 (GPU 0)]\n",
              "\t[.neg1]:[torch.cuda.LongTensor of size 256x62 (GPU 0)]\n",
              "\t[.neg2]:[torch.cuda.LongTensor of size 256x60 (GPU 0)]\n",
              "\t[.neg3]:[torch.cuda.LongTensor of size 256x80 (GPU 0)]\n",
              "\t[.neg4]:[torch.cuda.LongTensor of size 256x80 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3_GqGh3BN9Z",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "just use sentence embedding as an average over word embeddings **[5 points]**:  \n",
        "$z_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        "(implemented with ```nn.EmbeddingBag```)\n",
        "\n",
        "$p_t = softmax(W z_s + b), p_t \\in R^K$ topic weights for sentence $s$, with trainable matrix $W \\in R^{dxK}$ and bias vector $b \\in R^K$  \n",
        "$r_s = T^T p_t, r_s \\in R^d$ reconstructed sentence embedding as a weighted sum of topic embeddings   \n",
        "$T \\in R^{Kxd}$ trainable matrix of topic embeddings, K=number of topics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MW8c2395Tgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffbb06ec-6827-4dd3-b1ec-7fbbde0d400d"
      },
      "source": [
        "pad_id = TEXT.vocab.stoi['<pad>']\n",
        "pad_id"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyWZz9XSBNVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, emb_dim=300, topic_dim=5):\n",
        "      super(MyModel, self).__init__()\n",
        "      self.embedding = nn.EmbeddingBag(vocab_size, emb_dim)  ## how do I ignore the padding?\n",
        "      self.pt = nn.Linear(emb_dim, topic_dim)\n",
        "      self.soft = F.softmax\n",
        "      self.rs = nn.Linear(topic_dim, emb_dim, bias=False)\n",
        "\n",
        "    def forward(self, batch):\n",
        "      emb_x = self.embedding(batch.pos)\n",
        "      x = self.pt(emb_x)\n",
        "      x = self.soft(x)\n",
        "      x = self.rs(x) \n",
        "      \n",
        "      negs = [self.embedding(batch.neg0), \n",
        "              self.embedding(batch.neg1), \n",
        "              self.embedding(batch.neg2),\n",
        "              self.embedding(batch.neg3),\n",
        "              self.embedding(batch.neg4),]  ## how do I generalize this to different num_neg_samples?\n",
        "      negs = torch.stack(negs, dim=-1)\n",
        "\n",
        "      return x, emb_x, negs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrocE9_AfJYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c8f88935-e436-4d02-c93d-6362adcec8a8"
      },
      "source": [
        "model = MyModel(vocab_size)\n",
        "model = model.to(DEVICE)\n",
        "x, emb_x, negs = model(example_batch)\n",
        "x.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5-u-WIK_w5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cad48b71-24c9-49d7-fede-d22a39e05ead"
      },
      "source": [
        "emb_x.shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9mPzg4E_FRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc289c26-e668-4e80-8c48-c43a14eaa343"
      },
      "source": [
        "negs.shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 300, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywj6YL-4lJB4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0dc3468-6999-42f9-a9f1-29d0dfc14463"
      },
      "source": [
        "list(model.parameters())[0].shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([95799, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9xMJXlK_SWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec4c3f15-dd8c-49b3-defc-703a460a27d1"
      },
      "source": [
        "model.embedding.weight.shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([95799, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJoYWJZUBuqi",
        "colab_type": "text"
      },
      "source": [
        "## Loss\n",
        "**Training objective**:\n",
        "\n",
        "$$ J = \\sum_{s \\in D} \\sum_{i=1}^m max(0, 1-r_s^T z_s + r_s^T n_i) + \\lambda ||T^T T - I ||^2_F  $$\n",
        "where   \n",
        "$m$ random sentences are sampled as negative examples from dataset $D$ for each sentence $s$  \n",
        "$n_i = \\frac 1 n \\sum_{i=j}^n e_{w_j}$ average of word embeddings in the i-th sentence  \n",
        "$||T^T T - I ||_F$ regularizer, that enforces matrix $T$ to be orthogonal  \n",
        "$||A||^2_F = \\sum_{i=1}^N\\sum_{j=1}^M a_{ij}^2, A \\in R^{NxM}$ Frobenius norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gZd0mkCFNLX",
        "colab_type": "text"
      },
      "source": [
        "**TODO FIX LAMBDA ATTR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCpVzlItBv2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLoss(nn.Module):\n",
        "  def init(self, lmbd=0.01):\n",
        "    super(MyLoss, self).init()\n",
        "    self.lmbd = lmbd\n",
        "\n",
        "  def forward(self, vecs_true, negs, vecs_rec, T):\n",
        "    vecs_true = vecs_true.unsqueeze(1) ## add dimension for bmm\n",
        "    rs = vecs_rec.unsqueeze(1) ## add dimension for bmm\n",
        "    rsT = rs.permute(0, 2, 1) ## transpose\n",
        "    rsTzs = torch.bmm(rsT, vecs_true)\n",
        "    negs_losses = []\n",
        "    for ni in negs.permute(2, 0, 1):  ## so that we iterate over the neg samples\n",
        "      ni = ni.unsqueeze(1) ## add dimension for bmm\n",
        "      negs_losses.append(torch.bmm(rsT, ni))\n",
        "    losses = []\n",
        "    for n_loss in negs_losses:\n",
        "      tmp = (1 - rsTzs + n_loss).squeeze(1)\n",
        "      zeros = torch.zeros_like(tmp).to(DEVICE)\n",
        "      values, idx = torch.max(torch.stack([tmp, zeros]), 0)\n",
        "      losses.append(values)\n",
        "    losses = torch.stack(losses, dim=-1)\n",
        "    reg_0 = torch.mm(T.permute(1,0), T)\n",
        "    reg = (torch.norm(reg_0 - torch.eye(reg_0.shape[0]).to(DEVICE), p='fro')) #self.lmbd \n",
        "    return torch.sum(losses) + reg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YUFsnQFsh8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c85fdd7-eef7-473a-f0d0-94c71d81e6f7"
      },
      "source": [
        "criterion = MyLoss()\n",
        "criterion.to(DEVICE)\n",
        "criterion(emb_x, negs, x, model.embedding.weight)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1686e+08, device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V97gWI-5vuEB",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18SlWYJ-FTNx",
        "colab_type": "text"
      },
      "source": [
        "**TODO CHANGE THE EPOCH TRAIN TO THE NEW ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sm_8gYlvz6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 2\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yngRqDkbiEIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(data_iter, len_iter, n_epoch, model, criterion, optimizer=None):\n",
        "    train_losses = []\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm_notebook(data_iter, total=len_iter, desc=f\"Epoch {n_epoch + 1}\", leave=True)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        if optimizer:\n",
        "          optimizer.zero_grad()\n",
        "        vec_rec = model.forward(batch)\n",
        "        loss = criterion(batch.vecs, batch.negs, vec_rec,list(model.parameters())[-1])\n",
        "        loss.backward()\n",
        "        if optimizer:\n",
        "          optimizer.step()\n",
        "        loss_value = loss.detach().item()\n",
        "        total_loss += loss_value\n",
        "        train_losses.append(loss_value)\n",
        "        data_iter.set_postfix(loss = loss_value)\n",
        "        counter += 1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss, train_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwsvdLievvtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_train_losses = []\n",
        "total_valid_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss, train_losses = train_epoch(trn_itr, len(trn_itr), epoch, model, criterion, optimizer)\n",
        "    total_train_losses += train_losses\n",
        "    print('train', loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n5L5uHv1dLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smooth = lambda y: gaussian_filter1d(y, sigma=10)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.plot(range(len(total_train_losses)), smooth(total_train_losses), np.array(range(len(total_valid_losses)))*(len(total_train_losses)/len(total_valid_losses)), smooth(total_valid_losses))\n",
        "plt.legend(('train loss', 'valid loss by batch'),\n",
        "           loc='center', prop={'size': 18})\n",
        "plt.title('Smoothed training process', fontsize=20)\n",
        "plt.xlabel('Iterations', fontsize=16)\n",
        "plt.ylabel('Loss function (smoothed)', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpIq-PpWB11f",
        "colab_type": "text"
      },
      "source": [
        "## Topic Coherence"
      ]
    }
  ]
}