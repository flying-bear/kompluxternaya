{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "assignment_7_new",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e62f03956364327a35b10325e3bb996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8dde52a545de4da4a76f79d425d87fa3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_36506615e2c04684b356bc634988e7a8",
              "IPY_MODEL_d1213d1165de43899a9d6b21426ef707"
            ]
          }
        },
        "8dde52a545de4da4a76f79d425d87fa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36506615e2c04684b356bc634988e7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a739cb3b06342cc8ebad23fa624f240",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 235159,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 235159,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c878eec4eaf64c62853ef34211d0770d"
          }
        },
        "d1213d1165de43899a9d6b21426ef707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95841f604c9b4d71b3c8ccbf60fe392b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 235159/235159 [00:55&lt;00:00, 4207.66it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a826a9bdaecf4107827990b26a09d7d1"
          }
        },
        "9a739cb3b06342cc8ebad23fa624f240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c878eec4eaf64c62853ef34211d0770d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95841f604c9b4d71b3c8ccbf60fe392b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a826a9bdaecf4107827990b26a09d7d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70d7843ffd2c4a11b4b59260259a218a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_03034bf3e31748b8b7359672b389792c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4b2f258cd7c14be19bbc40f6fc3ffd59",
              "IPY_MODEL_524d49c51e07403ea271b9473d6dc26a"
            ]
          }
        },
        "03034bf3e31748b8b7359672b389792c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b2f258cd7c14be19bbc40f6fc3ffd59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5d2fbe5a1f4f4658b52022ea9a5d8593",
            "_dom_classes": [],
            "description": "Epoch 1",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 1647,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25326a0cc7e949a28896a97a8c29afe8"
          }
        },
        "524d49c51e07403ea271b9473d6dc26a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0dc0dc0dfb24df08b131f8138a408f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  2% 25/1647 [00:10&lt;10:55,  2.48it/s, loss=7.42]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ceeb3da06b6e42559f537445415d16a9"
          }
        },
        "5d2fbe5a1f4f4658b52022ea9a5d8593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25326a0cc7e949a28896a97a8c29afe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0dc0dc0dfb24df08b131f8138a408f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ceeb3da06b6e42559f537445415d16a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flying-bear/kompluxternaya/blob/master/assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2tslQ3iyL6x",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 7\n",
        "\n",
        "Train a Transformer model for Machine Translation from Russian to English.  \n",
        "Dataset: http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz   \n",
        "Make all source and target text to lower case.  \n",
        "Use following tokenization for english:  \n",
        "```\n",
        "import sentencepiece as spm\n",
        "\n",
        "...\n",
        "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "...\n",
        "TGT.build_vocab(..., min_freq=5)\n",
        "...\n",
        "\n",
        "```\n",
        "Score: corpus-bleu `nltk.translate.bleu_score.corpus_bleu`  \n",
        "Use last 1000 sentences for model evalutation (test dataset).  \n",
        "Use your target sequence tokenization for BLEU score.  \n",
        "Use max_len=50 for sequence prediction.  \n",
        "\n",
        "\n",
        "Hint: You may consider much smaller model, than shown in the example.  \n",
        "\n",
        "Baselines:  \n",
        "[4 point] BLEU = 0.05  \n",
        "[6 point] BLEU = 0.10  \n",
        "[9 point] BLEU = 0.15  \n",
        "\n",
        "[1 point] Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
        "\n",
        "\n",
        "Readings:\n",
        "1. BLUE score how to https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "1. Transformer code and comments http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK3biOEH5juC",
        "colab_type": "code",
        "outputId": "62b5fee4-8f20-4fc1-eb52-8f079843de73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!wget http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-22 13:20:54--  http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113157482 (108M) [application/x-gzip]\n",
            "Saving to: ‘training-parallel-nc-v13.tgz.2’\n",
            "\n",
            "training-parallel-n 100%[===================>] 107.92M  25.5MB/s    in 4.8s    \n",
            "\n",
            "2020-02-22 13:20:59 (22.3 MB/s) - ‘training-parallel-nc-v13.tgz.2’ saved [113157482/113157482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5OPsBlKJ4Mq",
        "colab_type": "code",
        "outputId": "80618301-f8f5-4d63-a414-ee780e6a59bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!gunzip -c training-parallel-nc-v13.tgz | tar xvf - "
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training-parallel-nc-v13/\n",
            "training-parallel-nc-v13/news-commentary-v13.ru-en.ru\n",
            "training-parallel-nc-v13/news-commentary-v13.cs-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.de-en.de\n",
            "training-parallel-nc-v13/news-commentary-v13.ru-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.zh-en.zh\n",
            "training-parallel-nc-v13/news-commentary-v13.zh-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.cs-en.cs\n",
            "training-parallel-nc-v13/news-commentary-v13.de-en.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vIToU2TqXqaN",
        "outputId": "ce9d07eb-c0fc-4a23-e3c1-309e9ae24b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!rm training-parallel-nc-v13/news-commentary-v13.cs-en.en\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.de-en.de\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.zh-en.zh\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.zh-en.en\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.cs-en.cs\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.de-en.en\n",
        "\n",
        "!ls training-parallel-nc-v13"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "news-commentary-v13.ru-en.en  news-commentary-v13.ru-en.ru\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQqHFHGaK2wg",
        "colab_type": "code",
        "outputId": "2c846050-8668-4a82-fdfd-89bca0014adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKRMVzaNyL60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy \n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sentencepiece as spm\n",
        "import seaborn\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu as bleu\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtext import datasets, data\n",
        "from torchtext.data import Field\n",
        "from torchtext.datasets import TranslationDataset\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "\n",
        "\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51VO9rWafLle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else tt.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfGeb46gsCHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnBG67LR6qZQ",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUM4KZe-61q4",
        "colab_type": "text"
      },
      "source": [
        "### ENGLISH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFbbPOfAyL64",
        "colab_type": "code",
        "outputId": "9366cb14-6fd6-4bb8-ae10-4aa47c889682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open('training-parallel-nc-v13/news-commentary-v13.ru-en.en') as f:\n",
        "    with open('text.en', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEbBI7se7r_a",
        "colab_type": "text"
      },
      "source": [
        "### RUSSIAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLbWtL_r6xSo",
        "colab_type": "code",
        "outputId": "90374b0d-a351-4652-857e-95e049f4af8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open('training-parallel-nc-v13/news-commentary-v13.ru-en.ru') as f:\n",
        "    with open('text.ru', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=text.ru --model_prefix=bpe_ru --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')  # CUSTOM"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuNE6h2H7uL2",
        "colab_type": "text"
      },
      "source": [
        "### Build vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4ZIZAztyL6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_ru = spm.SentencePieceProcessor()\n",
        "tok_ru.load('bpe_ru.model')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "SRC = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "fields = (('src', SRC), ('tgt', TGT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FQd5FoqyL7C",
        "colab_type": "code",
        "outputId": "c94527b3-ca48-4e05-f1c3-d329cd88a7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5e62f03956364327a35b10325e3bb996",
            "8dde52a545de4da4a76f79d425d87fa3",
            "36506615e2c04684b356bc634988e7a8",
            "d1213d1165de43899a9d6b21426ef707",
            "9a739cb3b06342cc8ebad23fa624f240",
            "c878eec4eaf64c62853ef34211d0770d",
            "95841f604c9b4d71b3c8ccbf60fe392b",
            "a826a9bdaecf4107827990b26a09d7d1"
          ]
        }
      },
      "source": [
        "with open('text.ru') as f:\n",
        "    src_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "with open('text.en') as f:\n",
        "    tgt_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "examples = [data.Example.fromlist(x, fields) for x in tqdm_notebook(zip(src_snt, tgt_snt), total=len(src_snt))]\n",
        "test = data.Dataset(examples[-1000:], fields)\n",
        "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e62f03956364327a35b10325e3bb996",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=235159), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUd3I7_MyL7G",
        "colab_type": "code",
        "outputId": "3b118d77-ea04-4eab-dc8f-7cbfc16fefe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('src: ' + \" \".join(train.examples[100].src))\n",
        "print('tgt: ' + \" \".join(train.examples[100].tgt))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src: ▁с ▁тех ▁пор ▁как ▁президент ▁сша ▁барак ▁обама ▁об ъ явил ▁о ▁стратегической ▁пере баланси ровке ▁в ▁сторону ▁азии , ▁их ▁задачи ▁ещ ё ▁больше ▁расши рились .\n",
            "tgt: ▁since ▁us ▁president ▁barack ▁obama ’ s ▁strategic ▁rebalance ▁to ▁asia , ▁they ▁have ▁been ▁doing ▁even ▁more .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqfl5Oz3yL7L",
        "colab_type": "code",
        "outputId": "cdfacf5d-e15d-4013-a5f9-fb059e42868d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train), len(valid), len(test)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210743, 23416, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05SoMD9MyL7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT.build_vocab(train, min_freq=5)\n",
        "SRC.build_vocab(train, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xkGFdvjCtp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eos = \"</s>\"\n",
        "sos = \"<s>\"\n",
        "pad = \"<pad>\"\n",
        "pad_idx = SRC.vocab.stoi[pad]\n",
        "sos_idx_tgt = TGT.vocab.stoi[sos]\n",
        "eos_idx_tgt = TGT.vocab.stoi[eos]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKfu910UqeRs",
        "colab_type": "text"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNfyxBkIggAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        src, tgt = batch.src, batch.tgt\n",
        "        src_mask, tgt_mask = batch.src_mask, batch.tgt_mask\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "    \n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "    \n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "    \n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / np.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        try:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        except:\n",
        "            pass\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "    \n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "    \n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * np.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "      return self.dropout(x)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyKYxVAdctus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, tgt=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if tgt is not None:\n",
        "            self.tgt = tgt[:, :-1]\n",
        "            self.tgt_y = tgt[:, 1:]\n",
        "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask\n",
        "\n",
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        nn.Linear(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model\n",
        "\n",
        "    \n",
        "class BucketIteratorWrapper(DataLoader):\n",
        "    __initialized = False\n",
        "\n",
        "    def __init__(self, iterator: data.Iterator):\n",
        "#         super(BucketIteratorWrapper,self).__init__()\n",
        "        self.batch_size = iterator.batch_size\n",
        "        self.num_workers = 1\n",
        "        self.collate_fn = None\n",
        "        self.pin_memory = False\n",
        "        self.drop_last = False\n",
        "        self.timeout = 0\n",
        "        self.worker_init_fn = None\n",
        "        self.sampler = iterator\n",
        "        self.batch_sampler = iterator\n",
        "        self.__initialized = True\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(\n",
        "            lambda batch: Batch(batch.src, batch.tgt, pad=TGT.vocab.stoi['<pad>']),\n",
        "            self.batch_sampler.__iter__()\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)\n",
        "    \n",
        "class MyCriterion:\n",
        "    def __init__(self, generator, pad_idx):\n",
        "        self.generator = generator\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
        "        self.criterion.cuda()\n",
        "        self.pad_idx = pad_idx\n",
        "        \n",
        "    def __call__(self, x, target):\n",
        "        ntokens = (target != self.pad_idx).data.sum()\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.reshape(-1, x.size(-1)), \n",
        "                              target.reshape(-1))  / ntokens\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rrqsKSNAaeW",
        "colab_type": "text"
      },
      "source": [
        "### TEXT HANDLER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyNnxT1OgmwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_text(seq, field):\n",
        "  \"\"\"gets a list of tokens given a list of items using a vocabulary of a given field\n",
        "  \n",
        "  :param seq: list of int, token ids\n",
        "  :param field: trained torchtext.data.Field\n",
        "  :return: list of str, tokens\n",
        "  \"\"\"\n",
        "  return [field.vocab.itos[v] for v in seq]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaOtzBR1g1DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_tok(tok_list, eos_token='</s>', sos_token='</s>', pad_token='<pad>'): \n",
        "  \"\"\"\n",
        "  removes paading and joins subword tokens\n",
        "\n",
        "  :param tok_list: list of str, subword tokens with \"_\" indicating a word beginning (space)\n",
        "  :param eos_token: str, end of sequence token, optional, default </s> \n",
        "  :param sos_token: str, start of sequence token, optional, default <s> \n",
        "  :param pad_token: str, pad token, optional, default <pad> \n",
        "  :return: str, joined sentence\n",
        "  \"\"\"\n",
        "  while tok_list[-1] == pad_token:\n",
        "    tok_list = tok_list[:-1]\n",
        "  if tok_list[-1] == eos_token:\n",
        "    return ''.join(tok_list[:-1]).replace('▁', ' ').strip() + ' ' + eos_token\n",
        "  else:\n",
        "    return ''.join(tok_list).replace('▁', ' ').strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pv4Me_J9LGt",
        "colab_type": "text"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og_B-a519OXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128  # CUSTOM\n",
        "num_epochs = 1  # CUSTOM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB1j0I-V_MMl",
        "colab_type": "text"
      },
      "source": [
        "### Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jef76IfkyL7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), \n",
        "                                    batch_sizes=(batch_size, batch_size, batch_size), \n",
        "                                    sort_key=lambda x: len(x.src),\n",
        "                                    shuffle=True,\n",
        "                                    device=DEVICE,\n",
        "                                    sort_within_batch=False)\n",
        "                                  \n",
        "train_iter = BucketIteratorWrapper(train_iter)\n",
        "valid_iter = BucketIteratorWrapper(valid_iter)\n",
        "test_iter = BucketIteratorWrapper(test_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCUPSpXJD_6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_train = len(list(iter(train_iter)))\n",
        "len_valid = len(list(iter(valid_iter)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13rQR2DG_Hpv",
        "colab_type": "code",
        "outputId": "b83a7fca-a26a-41f6-ab6b-0fa31376ad48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "example = next(iter(train_iter))\n",
        "example.src"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    2,   389,   916,  ...,     1,     1,     1],\n",
              "        [    2,     7,   366,  ...,     1,     1,     1],\n",
              "        [    2, 17863,  2596,  ...,     1,     1,     1],\n",
              "        ...,\n",
              "        [    2,     8,   145,  ...,     1,     1,     1],\n",
              "        [    2,    14,   394,  ...,  8940,     4,     3],\n",
              "        [    2,     5,    24,  ...,     1,     1,     1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPWm7xcVgAN0",
        "colab_type": "code",
        "outputId": "ed0fcce8-9edf-4491-e583-ca893925e364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "join_tok(to_text(example.src[0], SRC))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> проблема европейских лидеров состоит в том, что их собственное непризнание со стороны избирателей не обязательно имеет непосредственную связь с расширением политики скептицизма в отношении евросоюза. </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_WjGHF42HKy",
        "colab_type": "code",
        "outputId": "bb077d1f-f478-4637-95ab-f52277d5af8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "join_tok(to_text(example.tgt[0], TGT))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<s> the problem for europe's leaders is that their own repudiation by voters does not necessarily carry the same message as the rise of the euroskeptic parties. the record slump in the vote for gerhard schröder's governing social democrats in germany has\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zzs5c0VuaYB",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwDrIzx3rqcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TGT.vocab), N=2)\n",
        "model = model.to(DEVICE)\n",
        "criterion = MyCriterion(model.generator, pad_idx)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
        "\n",
        "# # share weights\n",
        "# <TODO>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2If4ooDc7QP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(data_iter, len_iter, n_epoch, model, criterion, optimizer=None):\n",
        "    train_losses = []\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm_notebook(data_iter, total=len_iter, desc=f\"Epoch {n_epoch + 1}\", leave=True)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        if optimizer:\n",
        "          optimizer.zero_grad()\n",
        "        pred = model.forward(batch)\n",
        "        loss = criterion(pred, batch.tgt_y)\n",
        "        loss.backward()\n",
        "        if optimizer:\n",
        "          optimizer.step()\n",
        "        loss_value = loss.detach().item()\n",
        "        total_loss += loss_value\n",
        "        train_losses.append(loss_value)\n",
        "        data_iter.set_postfix(loss = loss_value)\n",
        "        counter += 1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss, train_losses\n",
        "\n",
        "\n",
        "def valid_epoch(data_iter, len_iter, n_epoch, model, criterion):\n",
        "    valid_losses = []\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm_notebook(data_iter, total=len_iter, desc=f\"Eval epoch {n_epoch + 1}\", leave=True)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        with torch.no_grad():\n",
        "            pred = model.forward(batch)\n",
        "            loss = criterion(pred, batch.tgt_y)\n",
        "            loss_value = loss.detach().item()\n",
        "            total_loss += loss_value\n",
        "            valid_losses.append(loss_value)\n",
        "            data_iter.set_postfix(loss = loss_value)\n",
        "            counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss, valid_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqquZCDbuiqy",
        "colab_type": "code",
        "outputId": "c0f8a0f1-8755-4d15-a238-556f814b695a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 22 13:23:17 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0    28W /  70W |   5021MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WapbwUQYrsxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420,
          "referenced_widgets": [
            "70d7843ffd2c4a11b4b59260259a218a",
            "03034bf3e31748b8b7359672b389792c",
            "4b2f258cd7c14be19bbc40f6fc3ffd59",
            "524d49c51e07403ea271b9473d6dc26a",
            "5d2fbe5a1f4f4658b52022ea9a5d8593",
            "25326a0cc7e949a28896a97a8c29afe8",
            "e0dc0dc0dfb24df08b131f8138a408f9",
            "ceeb3da06b6e42559f537445415d16a9"
          ]
        },
        "outputId": "006c9c55-b9b1-4abc-a7c5-040172a35dce"
      },
      "source": [
        "total_train_losses = []\n",
        "total_valid_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss, train_losses = train_epoch(train_iter, len_train, epoch, model, criterion, optimizer)\n",
        "    total_train_losses += train_losses\n",
        "    print('train', loss)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss, valid_losses = valid_epoch(valid_iter, len_valid, epoch, model, criterion)\n",
        "        total_valid_losses += valid_losses\n",
        "        if scheduler:\n",
        "          if type(scheduler) == torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
        "            scheduler.step(loss)\n",
        "          else:\n",
        "            scheduler.step() \n",
        "        print('valid', loss)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70d7843ffd2c4a11b4b59260259a218a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch 1', max=1647, style=ProgressStyle(description_width='in…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-5b621b484388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_train_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-172-6601348adc46>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(data_iter, len_iter, n_epoch, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Z54H8kZLEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(total_train_losses))\n",
        "print(len(total_valid_losses))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7aJ3_GWZlSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smooth = lambda y: gaussian_filter1d(y, sigma=100)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.plot(range(len(total_train_losses)), smooth(total_train_losses), np.array(range(len(total_valid_losses)))*(len(total_train_losses)/len(total_valid_losses)), smooth(total_valid_losses))\n",
        "plt.legend(('train loss', 'valid loss by batch'),\n",
        "           loc='center', prop={'size': 18})\n",
        "plt.title('Smoothed training process', fontsize=20)\n",
        "plt.xlabel('Iterations', fontsize=16)\n",
        "plt.ylabel('Loss function (smoothed)', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE6wpimPasuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'model_1.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icYGV9C0bBs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_loaded = make_model(len(SRC.vocab), len(TGT.vocab), N=2) \n",
        "model_loaded.load_state_dict(torch.load('model_1.pt'))\n",
        "model_loaded = model_loaded.to(DEVICE)\n",
        "model_loaded.eval()\n",
        "with torch.no_grad():\n",
        "  print(model_loaded.forward(example)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tov2kNDrxoT",
        "colab_type": "text"
      },
      "source": [
        "## PREDICT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7PLh_wDwoJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(model, src, src_mask, max_len=10, k=5, offset=0):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    start_token = TGT.vocab.stoi[\"<s>\"]\n",
        "    end_token = TGT.vocab.stoi[\"</s>\"]\n",
        "    ys = torch.ones(1, 1).fill_(start_token).type_as(src.data)\n",
        "    beam = [(ys, 0)]\n",
        "    for i in range(max_len):\n",
        "        candidates= []\n",
        "        candidates_proba = []\n",
        "        prev_prob = None\n",
        "        for snt, snt_proba in beam:\n",
        "            if snt[0][-1] == end_token:\n",
        "                candidates.append(snt)\n",
        "                candidates_proba.append(snt_proba)\n",
        "            else:\n",
        "                proba = model.decode(memory, src_mask, snt,\n",
        "                                     subsequent_mask(snt.size(1)).type_as(src.data))\n",
        "                proba = proba[0][i]\n",
        "                best_k = torch.argsort(-proba)[:k].tolist()\n",
        "                proba = proba.tolist()\n",
        "                prev_prob = proba\n",
        "                for tok in best_k:\n",
        "                    candidates.append(torch.cat([snt, torch.ones(1, 1).type_as(src.data).fill_(tok)], dim=1))\n",
        "                    candidates_proba.append(snt_proba + np.log(proba[tok])) \n",
        "         \n",
        "        best_candidates = np.argsort(-np.array(candidates_proba))[offset:k+offset]\n",
        "        beam = [(candidates[j], candidates_proba[j]) for j in best_candidates]\n",
        "    return beam "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q0KIpfV7Bsv1",
        "colab": {}
      },
      "source": [
        "def soft_with_temp(logits_tsr, tau=1):\n",
        "  \"\"\"\n",
        "  computes softmax with temperature tau\n",
        "  \n",
        "  :param logits_tsr: torch tensor of logits\n",
        "  :param tau: float, temperature, optional, default 1\n",
        "  :return: np.array, softmax with temperature\n",
        "  \"\"\"\n",
        "  logits_tsr = logits_tsr.detach()\n",
        "  out = torch.exp(logits_tsr/tau)/torch.sum(torch.exp(logits_tsr/tau))  # F.softmax(logits/tau, dim=0)\n",
        "  out = out.squeeze()\n",
        "  if np.sum(out.numpy()) != 1:\n",
        "    return out.numpy()/np.sum(out.numpy())\n",
        "  else:\n",
        "    return out.numpy()\n",
        "\n",
        "def select_top_k(logits_tsr, k=0.2):\n",
        "  \"\"\"\n",
        "  select top-k logits and thir ids\n",
        "\n",
        "  :param logits_tsr: torch tensor of logits\n",
        "  :param k: float, k threshold, optional, default 0.2\n",
        "  :return top_k_tsr: torch tensor of top_k logits\n",
        "  :return top_k_ids: torch tensor of ints, ids of top_k logits in logits_tsr\n",
        "  \"\"\"\n",
        "  logits_tsr = logits_tsr.detach().squeeze()\n",
        "  argsorted_tsr = torch.argsort(logits_tsr, descending=True)\n",
        "  kth_id = int(np.floor(len(logits_tsr) * k))\n",
        "  if kth_id == 0:\n",
        "    kth_id = 1\n",
        "  top_k_ids = argsorted_tsr[:kth_id]\n",
        "  top_k_tsr = logits_tsr[top_k_ids]\n",
        "  return top_k_tsr, top_k_ids\n",
        "\n",
        "def sample_top_k(logits_tsr, k=0.2):\n",
        "  \"\"\"\n",
        "  top_k sampling\n",
        "\n",
        "  :param logits_tsr: torch tensor of logits\n",
        "  :param k: float, k threshold, optional, default 0.2\n",
        "  :return: int, id of the element selected\n",
        "  \"\"\"\n",
        "  top_k_logits, top_k_ids = select_top_k(logits_tsr, k=k)\n",
        "  soft_probs = soft_with_temp(top_k_logits)\n",
        "  if len(top_k_ids) == 1:\n",
        "    return top_k_ids.numpy()[0]\n",
        "  return np.random.choice(top_k_ids.numpy(), 1, p=soft_probs)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyPEkfxS_xPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_k(pred):\n",
        "    top_k = 300\n",
        "    top_k = min(top_k, pred.size(-1))  # Safety check\n",
        "    # Remove all tokens with a probability less than the last token of the top-k\n",
        "    indices_to_remove = pred < torch.topk(pred, top_k)[0][..., -1, None]\n",
        "    pred[indices_to_remove] = -float('Inf')\n",
        "    probs = torch.softmax(pred, dim=-1)\n",
        "    prev = torch.multinomial(probs, num_samples=1)\n",
        "    return prev\n",
        "\n",
        "def decode(model, prob_func, src, src_mask, max_len=10): #, start_symbol, target):\n",
        "    start_symbol, end_token = TGT.vocab.stoi[\"<s>\"], TGT.vocab.stoi[\"</s>\"]\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        next_word = prob_func(prob)[0]\n",
        "        next_word += 2\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        if TGT.vocab.itos[ys[0][-1]] == \"</s>\":\n",
        "\n",
        "          break\n",
        "    return ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CidiXiq_8aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_text(decode(model, top_k, example.src, example.src_mask)[0], TGT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN2zh83HdAr2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "89d5c988-e87c-427f-cb8f-30e4dce676b6"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(valid_iter):\n",
        "        src = batch.src[:1]\n",
        "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "        beam = beam_search(model, src, src_key_padding_mask)\n",
        "        \n",
        "        seq = []\n",
        "        for i in range(1, src.size(1)):\n",
        "            sym = SRC.vocab.itos[src[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_ru.decode_pieces(seq)\n",
        "        print(\"\\nSource:\", seq)\n",
        "        \n",
        "        print(\"Translation:\")\n",
        "        for pred, pred_proba in beam:                \n",
        "            seq = []\n",
        "            for i in range(1, pred.size(1)):\n",
        "                sym = TGT.vocab.itos[pred[0, i]]\n",
        "                if sym == \"</s>\": break\n",
        "                seq.append(sym)\n",
        "            seq = tok_en.decode_pieces(seq)\n",
        "            print(f\"pred {pred_proba:.2f}:\", seq)\n",
        "                \n",
        "        seq = []\n",
        "        for i in range(1, batch.tgt.size(1)):\n",
        "            sym = TGT.vocab.itos[batch.tgt[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_en.decode_pieces(seq)\n",
        "        print(\"Target:\", seq)\n",
        "        break"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Source: сосуществование\n",
            "Translation:\n",
            "pred 12.79: before before before before before before before before before before\n",
            "pred 12.74: before before before before before before before before before good\n",
            "pred 12.74: before before before before before good before before before before\n",
            "pred 12.74: before before before before before before good before before before\n",
            "pred 12.74: before before before before before before before before good before\n",
            "Target: coexistence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDdoUcTEr3WG",
        "colab_type": "text"
      },
      "source": [
        "### BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aosFUSfzdI1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        <TODO>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2byJN9pydMvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_bleu(references, hypotheses, \n",
        "            smoothing_function=translate.bleu_score.SmoothingFunction().method3,\n",
        "            auto_reweigh=True\n",
        "           )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}