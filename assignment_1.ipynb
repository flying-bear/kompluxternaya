{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flying-bear/kompluxternaya/blob/master/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSsqcR7Tb9DB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Assignment 1\n",
        "\n",
        "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
        "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
        "2. Tokenize text by BPE with vocab_size = 100\n",
        "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
        "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
        "5. Calculate perplexity of the language model for the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1v5OMqAKhAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from math import exp, log\n",
        "from sklearn.base import TransformerMixin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn5f4JDpb9DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bf83803b-3237-498a-df92-34e0651b51bb"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vvv269v5pUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d06e61f4-2d4b-448c-e12f-ccb67bc09cb7"
      },
      "source": [
        "root_path = 'gdrive/My Drive/studies/HSE/prog/kompluxternaya'\n",
        "with open(root_path+'/'+'peace.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()[2:]\n",
        "print(len(text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3227579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Fi2UAjb9Dc",
        "colab_type": "code",
        "outputId": "98637c6b-ce99-46b1-d358-b720c669d170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # TODO\n",
        "    # make lowercase\n",
        "    text = text.lower()\n",
        "    # replace all punctuation except dots with spaces\n",
        "    pattern = '\\]|!|\"|#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|/|:|;|<|=|>|\\?|@|\\[|\\\\|^|_|`|{|\\||}|~|”|“|—|‘|’'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    # collapse multiple spaces into one '   ' -> ' '\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "text = preprocess_text(text)\n",
        "#assert len(text) == 3141169\n",
        "len(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3141168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_YETckkb9Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text.split('.')\n",
        "text = [x.strip() for x in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeRdojT9b9Do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_bigrams(l, bigrams=Counter()):\n",
        "  \"\"\"\n",
        "  transforms a list of lists into a counter of their pairs (tuples) across the lists\n",
        "  l: a list of lists of integers or strings\n",
        "  \"\"\"\n",
        "  for i in range(len(l)):\n",
        "    bigrams.update((x, y) for x, y in zip(*[l[i][j:] for j in range(2)]))\n",
        "  return bigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrKNPKbSZKbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_token(text, token, token_id):\n",
        "  \"\"\"\n",
        "  modifies text, all instances of the token (element pairs) in text are replaced by token_id\n",
        "  text: list of lists of integers\n",
        "  token: integer tuple\n",
        "  token_id: the id of the given token, integer\n",
        "  \"\"\"\n",
        "  text_new = [inner_list[:] for inner_list in text]\n",
        "  for i, sent in enumerate(text):\n",
        "    deletions = 0\n",
        "    for j, (v, w) in enumerate(zip(sent[:-1], sent[1:])):\n",
        "      if (v, w) == token:\n",
        "        text_new[i][j-deletions] = token_id\n",
        "        del text_new[i][j-deletions+1]\n",
        "        deletions += 1\n",
        "  return text_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7xGXMYyrtyh",
        "colab_type": "code",
        "outputId": "cc6198bd-7efd-44fa-d3e1-37d227ae8693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "t = ['бля бля бл', ' ', 'бля']\n",
        "test_itos = list(set(''.join(t))) # list letters\n",
        "test_stoi = dict(zip(test_itos, range(len(test_itos)))) # dict of letters and their ids\n",
        "t = [[test_stoi[x] for x in t[i]] for i in range(len(t))] # replace letters with thir ids (str to list)\n",
        "test_vocab_size = 12\n",
        "while len(test_itos) < test_vocab_size:\n",
        "    bigrams = list_to_bigrams(t) # get text bigrams\n",
        "    if bigrams.most_common(1):\n",
        "      new_token = bigrams.most_common(1)[0][0] # find most common bigram\n",
        "      new_id = len(test_itos)\n",
        "      test_itos.append(new_token)\n",
        "      test_stoi[new_token] = new_id\n",
        "      # find occurences of the new_token in the text and replace them with new_id\n",
        "      t = update_token(t, new_token, new_id)\n",
        "    else:\n",
        "      break\n",
        "print(t)\n",
        "print(test_itos)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10, 10, 4], [2], [7]]\n",
            "['я', 'б', ' ', 'л', (1, 3), (1, 3), (0, 2), (4, 0), (4, 0), (4, 0), (4, 6), (4, 6)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUbIJjBAWGqR",
        "colab_type": "code",
        "outputId": "c53efea9-3bdd-4587-be81-fdeb52019f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def recursive_token_lookup(tok, itos): #tok int or tuple\n",
        "  if type(tok) == int:\n",
        "    content = itos[tok]\n",
        "    if type(content) == str:\n",
        "      return content\n",
        "    else:\n",
        "      return recursive_token_lookup(content, itos)\n",
        "  elif type(tok) == tuple:\n",
        "    return recursive_token_lookup(tok[0], itos) + recursive_token_lookup(tok[1], itos)\n",
        "  \n",
        "recursive_token_lookup(7, test_itos)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'бля'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1CNokZ4ub9Dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BPE(TransformerMixin):\n",
        "    def __init__(self, vocab_size=100):\n",
        "        super(BPE, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # index to token\n",
        "        self.itos = []\n",
        "        # token to index\n",
        "        self.stoi = {}\n",
        "\n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        fits \n",
        "        - self.itos (a list of strings (symbols) and integer tuples (most frequent bigrams while vocabulary does not exceed vocab_size\n",
        "                      each symbol/bigram is indexed by its place in the list) )\n",
        "        - self.stoi (a dict of symbols and bigrams to thir indicies in the self.stoi list)\n",
        "        - text (symbols replaced by thir ids, bigrams of ids replaced by their ids\n",
        "        text: list of strings\n",
        "        \"\"\"\n",
        "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
        "        self.itos = list(set(''.join(text))) # list letters\n",
        "        self.stoi = dict(zip(self.itos, range(len(self.itos)))) # dict of letters and their ids\n",
        "        text = [[self.stoi[x] for x in text[i]] for i in range(len(text))] # replace letters with thir ids (str to list)\n",
        "        \n",
        "        while len(self.itos) < self.vocab_size:\n",
        "            bigrams = list_to_bigrams(text) # get text bigrams\n",
        "            if bigrams.most_common(1):\n",
        "              new_token = bigrams.most_common(1)[0][0] # find most common bigram\n",
        "              new_id = len(self.itos)\n",
        "              self.itos.append(new_token)\n",
        "              self.stoi[new_token] = new_id\n",
        "              # find occurences of the new_token in the text and replace them with new_id\n",
        "              text = update_token(text, new_token, new_id)\n",
        "            else:\n",
        "              break\n",
        "        return self\n",
        "    \n",
        "    def transform(self, text):\n",
        "        \"\"\"\n",
        "        convert text to a sequence of symbol ids then replaces bigrams of ids with their indicies in self.stoi\n",
        "        text: list of strings\n",
        "        \"\"\"\n",
        "        text_in_vocabulary = [[symbol for symbol in sent if symbol in self.itos] for sent in text] # exclude out of vocabulary symbols\n",
        "        text = [[self.stoi[letter] for letter in sent] for sent in text_in_vocabulary] # tokenize text by symbols using self.stoi\n",
        "        for token_id, token in enumerate(self.itos): # find occurences of a complex token in the text and replace them with token_id\n",
        "            text = update_token(text, token, token_id)    \n",
        "        return text\n",
        "    \n",
        "    def decode_token(self, tok):\n",
        "        \"\"\"\n",
        "        returns a text coded by the tok\n",
        "        tok: either an int - id, or a tuple - pair of ids)\n",
        "        \"\"\"\n",
        "        def recursive_token_lookup(token): #token int or tuple\n",
        "          if type(token) == int:\n",
        "            content = self.itos[token]\n",
        "            if type(content) == str:\n",
        "              return content # only returns strings\n",
        "            else:\n",
        "              return recursive_token_lookup(content) # continue lookup on the tuple that was found\n",
        "          elif type(token) == tuple:\n",
        "            return recursive_token_lookup(token[0]) + recursive_token_lookup(token[1]) # concatenate string results\n",
        "        return recursive_token_lookup(tok)\n",
        "            \n",
        "    def decode(self, text):\n",
        "        \"\"\"\n",
        "        convert token ids into text\n",
        "        text: tokenized text, list of lists of integers\n",
        "        \"\"\"\n",
        "        return ''.join(map(self.decode_token, text))\n",
        "        \n",
        "        \n",
        "vocab_size = 100\n",
        "bpe = BPE(vocab_size)\n",
        "tokenized_text = bpe.fit_transform(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udBjXkGGt5lV",
        "colab_type": "code",
        "outputId": "cf37b6aa-d2bf-454c-c1b0-2e8098dc3136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_text = ['бля бля бл', ' ', 'бля']\n",
        "test_vocab_size = 12\n",
        "test_bpe = BPE(test_vocab_size)\n",
        "test_tokenized_text = test_bpe.fit_transform(test_text)\n",
        "test_tokenized_text\n",
        "test_bpe.decode(test_tokenized_text[0])\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'бля бля бл'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DTU3BJQb9D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert bpe.decode(tokenized_text[0]) == text[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izHAJ88h2xuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_trigrams(l, trigrams=Counter()):\n",
        "  \"\"\"\n",
        "  transforms a list of lists into a counter of their pairs (tuples) across the lists\n",
        "  l: a list of lists of integers or strings\n",
        "  \"\"\"\n",
        "  for i in range(len(l)):\n",
        "    trigrams.update((x, y, z) for x, y, z in zip(*[l[i][j:] for j in range(3)]))\n",
        "  return trigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcr54ypQAZLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth(count, delta=1, tau=1):\n",
        "  \"\"\"\n",
        "  laplace smoothing\n",
        "  count: int\n",
        "  delta: int or float\n",
        "  tau: int or float\n",
        "  \"\"\"\n",
        "  return (count + delta) ** (1 / tau)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFtNaJDbCFu8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "08fad384-47c3-497f-bc47-2cd88d2fe1c1"
      },
      "source": [
        "test_counts = list_to_trigrams(test_tokenized_text)\n",
        "unsmoothed = [test_counts[(1, 0, x)] for x in range(len(set([item for sublist in test_tokenized_text for item in sublist])))]\n",
        "smoothed = [smooth(test_counts[(1, 0, x)]) for x in range(len(set([item for sublist in test_tokenized_text for item in sublist])))]\n",
        "print('non-smooth:')\n",
        "print(unsmoothed)\n",
        "print('____')\n",
        "print('smooth:')\n",
        "print(smoothed)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "non-smooth:\n",
            "[0, 0, 0, 0]\n",
            "____\n",
            "smooth:\n",
            "[1.0, 1.0, 1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1MWgL6sb9EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_token = vocab_size\n",
        "end_token = vocab_size + 1\n",
        "        \n",
        "    \n",
        "class LM:\n",
        "    def __init__(self, vocab_size, delta=1):\n",
        "        self.delta = delta\n",
        "        self.vocab_size = vocab_size + 2\n",
        "        self.counts = Counter() # TODO create array for storing 3-gram counts (empty 3-gram counter)\n",
        "\n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        train language model on text\n",
        "        text: list of lists\n",
        "        \"\"\"\n",
        "        text = [[start_token]+sent+[end_token] for sent in text]\n",
        "        self.counts = list_to_trigrams(text, self.counts) # count 3-grams in the text  \n",
        "        return self\n",
        "        \n",
        "    def get_proba(self, a, b, c, tau=1):\n",
        "        \"\"\"\n",
        "        get Laplace smoothed probability of 3-gram (a,b,c)\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        c: third token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        trigram = (a, b, c)\n",
        "        counts_abx = [smooth(self.counts[(a, b, x)]) for x in range(self.vocab_size)] # smooth counts for every possible trigram starting with given bigram\n",
        "        result = smooth(self.counts[trigram]) / sum(counts_abx) # approximate probability by counters\n",
        "        return result\n",
        "            \n",
        "    def infer(self, a, b, tau=1):\n",
        "        \"\"\"\n",
        "        return vector of probabilities of size self.vocab_size for 3-grams which start with (a,b) tokens\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        result = [self.get_proba(a,b,x, tau) for x in range(self.vocab_size)]\n",
        "        return result\n",
        "    \n",
        "lm = LM(vocab_size, 1).fit(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjnXii5eFYnp",
        "colab_type": "code",
        "outputId": "255f1acd-614d-4f87-aca3-89f0ab05f4c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "l = [[0,1,2,3], [3,1,2,0,4], [4,1,2,3,2,3,1,2,3,2]]\n",
        "test_lm = LM(len(set([item for sublist in l for item in sublist])), 1).fit(l)\n",
        "test_lm.infer(1,2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18181818181818182,\n",
              " 0.09090909090909091,\n",
              " 0.09090909090909091,\n",
              " 0.36363636363636365,\n",
              " 0.09090909090909091,\n",
              " 0.09090909090909091,\n",
              " 0.09090909090909091]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f612_bg6uNdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "464dd07d-71a2-4e11-8aea-00e992f08991"
      },
      "source": [
        "test_l = test_tokenized_text\n",
        "real_test_lm = LM(len(set([item for sublist in test_l for item in sublist])), 1).fit(test_l)\n",
        "real_test_lm.infer(3,0)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.125, 0.125, 0.375, 0.125, 0.125, 0.125]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb7zr3wBG2O7",
        "colab_type": "code",
        "outputId": "8e3ae68f-9503-4795-fff5-46b8231ce324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "lm.infer(1,57)[5:15]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.00016960651289009497,\n",
              " 0.00016960651289009497,\n",
              " 0.00033921302578018993,\n",
              " 0.00016960651289009497,\n",
              " 0.05529172320217096,\n",
              " 0.018317503392130258,\n",
              " 0.00016960651289009497,\n",
              " 0.00016960651289009497,\n",
              " 0.029511533242876527,\n",
              " 0.005257801899592945]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgGe9n4QxoBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def enum_sort_tuple(arr):\n",
        "  \"\"\"\n",
        "  makes list of tuples of ids and their values in the decreasing order of the values from the list input\n",
        "  arr: flat list\n",
        "  \"\"\"\n",
        "  return sorted(enumerate(arr), key=lambda x:x[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAQnY_A-5eJL",
        "colab_type": "code",
        "outputId": "2351bdea-c001-459b-8659-ab8b8ce54a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "enum_sort_tuple([1, 2, 3, 1, 31, 0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, 31), (2, 3), (1, 2), (0, 1), (3, 1), (5, 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek7jeggLb9ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
        "    \"\"\"\n",
        "    generate prediction list from language model *lm* conditioned on input_seq\n",
        "    input_seq: list of token ids for conditioning\n",
        "    lm: language model\n",
        "    max_len: max generated list length\n",
        "    k: size of beam\n",
        "    tau: temperature\n",
        "    \"\"\"\n",
        "    input_seq = [start_token] + input_seq\n",
        "    x, y = input_seq[-2:]\n",
        "    beam = [([y, token], proba) for token, proba in enum_sort_tuple(lm.infer(x, y, tau))[:k]] # store in beam tuples of current sequences and their log probabilities\n",
        "    for i in range(max_len):\n",
        "      candidates = []\n",
        "      candidates_log_proba = []\n",
        "      for snt, snt_log_proba in beam:\n",
        "          if snt[-1] == end_token:# TODO process terminal token \n",
        "            continue\n",
        "          else:\n",
        "            a, b = snt[-2:]\n",
        "            proba = lm.infer(a, b, tau) # probability vector of the next token\n",
        "            best_k = enum_sort_tuple(proba)[:k] # top-k most probable (token, probability) pairs\n",
        "            candidates += [snt + [token] for token, proba in best_k] # update candidates' sequences\n",
        "            candidates_log_proba += [snt_log_proba + log(proba) for token, proba in best_k] # update corresponding probabilities    \n",
        "      beam = [(candidates[index], log_proba) for index, log_proba in enum_sort_tuple(candidates_log_proba)[:k]] # select top-k most probable sequences from candidates\n",
        "    return [(snt[1:], exp(snt_log_proba)) for snt, snt_log_proba in beam]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXXNH0ieb9EZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0ff12a83-f054-4d93-e222-7ab8af06b621"
      },
      "source": [
        "input1 = 'horse '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "for snt, snt_proba in result:\n",
        "  print(f'hypothesis: \"{bpe.decode(snt)}\"')\n",
        "  print(f'probability: {snt_proba}')\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hypothesis: \"with his with hi\"\n",
            "probability: 7.659747275838157e-06\n",
            "hypothesis: \"that it was not \"\n",
            "probability: 5.8625778068365075e-06\n",
            "hypothesis: \"with his with a \"\n",
            "probability: 5.6960700417323595e-06\n",
            "hypothesis: \"that he was now \"\n",
            "probability: 4.65697608804872e-06\n",
            "hypothesis: \"that it was now\"\n",
            "probability: 4.26326906790895e-06\n",
            "hypothesis: \"that he was nown\"\n",
            "probability: 4.042741995970955e-06\n",
            "hypothesis: \"with his with an\"\n",
            "probability: 3.603427822099503e-06\n",
            "hypothesis: \"that he was all \"\n",
            "probability: 3.5799361815179967e-06\n",
            "hypothesis: \"with his with her\"\n",
            "probability: 3.034782911918333e-06\n",
            "hypothesis: \"with his fack \"\n",
            "probability: 3.0185801711278287e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxqKvnvAb9Ef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "61538ed8-9045-49c5-be3b-19e4465ef925"
      },
      "source": [
        "input1 = 'her'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "for snt, snt_proba in result:\n",
        "  print(f'hypothesis: \"{bpe.decode(snt)}\"')\n",
        "  print(f'probability: {snt_proba}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hypothesis: \"ed to his with \"\n",
            "probability: 8.643180715488237e-06\n",
            "hypothesis: \"ed to his with a\"\n",
            "probability: 5.963937713775946e-06\n",
            "hypothesis: \" and with his he\"\n",
            "probability: 5.268217738063301e-06\n",
            "hypothesis: \" and with himper\"\n",
            "probability: 4.874363695393653e-06\n",
            "hypothesis: \"ed to his with h\"\n",
            "probability: 4.748266956771256e-06\n",
            "hypothesis: \" and with his wi\"\n",
            "probability: 4.584553471805025e-06\n",
            "hypothesis: \" and with his fa\"\n",
            "probability: 4.343423544918241e-06\n",
            "hypothesis: \" and with his st\"\n",
            "probability: 4.109638533630364e-06\n",
            "hypothesis: \"ed to his with the \"\n",
            "probability: 3.86630974090511e-06\n",
            "hypothesis: \" and with his a \"\n",
            "probability: 3.826087971181735e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA7l1zyNb9Ek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "41ca7630-aba2-472b-ab20-bd24f636bc09"
      },
      "source": [
        "input1 = 'what'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
        "for snt, snt_proba in result:\n",
        "  print(f'hypothesis: \"{bpe.decode(snt)}\"')\n",
        "  print(f'probability: {snt_proba}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hypothesis: \" that he was all \"\n",
            "probability: 4.856790571285655e-06\n",
            "hypothesis: \" and with his he\"\n",
            "probability: 4.7857293296851605e-06\n",
            "hypothesis: \" and with himper\"\n",
            "probability: 4.427946311340873e-06\n",
            "hypothesis: \" and with his wi\"\n",
            "probability: 4.1646782848411585e-06\n",
            "hypothesis: \" and with his fa\"\n",
            "probability: 3.945632182203954e-06\n",
            "hypothesis: \" and with his st\"\n",
            "probability: 3.733258312901353e-06\n",
            "hypothesis: \" and with his a \"\n",
            "probability: 3.4756766580364223e-06\n",
            "hypothesis: \" and with himpl\"\n",
            "probability: 3.3557341809769155e-06\n",
            "hypothesis: \" and with his sa\"\n",
            "probability: 3.28192409895358e-06\n",
            "hypothesis: \" and with his for\"\n",
            "probability: 2.995224430286215e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZSZy_cCb9Eq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ff1a07b7-5a71-4d11-c5d4-bb568714ec0f"
      },
      "source": [
        "input1 = 'gun '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "for snt, snt_proba in result:\n",
        "  print(f'hypothesis: \"{bpe.decode(snt)}\"')\n",
        "  print(f'probability: {snt_proba}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hypothesis: \"expression of \"\n",
            "probability: 1.1965771057691515e-05\n",
            "hypothesis: \"expressible t\"\n",
            "probability: 9.507365061191772e-06\n",
            "hypothesis: \"expression of the \"\n",
            "probability: 8.647532024886306e-06\n",
            "hypothesis: \"expection of \"\n",
            "probability: 6.809722104443703e-06\n",
            "hypothesis: \"expressible an\"\n",
            "probability: 6.778793922553773e-06\n",
            "hypothesis: \"expressible w\"\n",
            "probability: 6.693526074471334e-06\n",
            "hypothesis: \"expressible s\"\n",
            "probability: 6.082439829880531e-06\n",
            "hypothesis: \"expressible of\"\n",
            "probability: 5.7840023615920045e-06\n",
            "hypothesis: \"expressible b\"\n",
            "probability: 5.7555797455645205e-06\n",
            "hypothesis: \"expection of the \"\n",
            "probability: 4.921311772959316e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHhRyjYfb9E3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec4eb10b-c7be-42ee-b47a-7f8297eccbf6"
      },
      "source": [
        "def perplexity(snt, lm):\n",
        "    \"\"\"\n",
        "    snt: sequence of token ids\n",
        "    lm: language model\n",
        "    \"\"\"\n",
        "    n = len(snt)\n",
        "    p = 0\n",
        "    for trigram in list_to_trigrams([snt]):\n",
        "      p += log(lm.get_proba(*trigram))\n",
        "    result = exp(p) ** (-1/n) # perplexity for the sentence\n",
        "    return result\n",
        "\n",
        "perplexity(tokenized_text[0], lm)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.537062655934571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}