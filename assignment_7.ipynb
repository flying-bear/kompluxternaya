{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "assignment_7",
      "provenance": [],
      "collapsed_sections": [
        "2rrqsKSNAaeW"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ddb4b7ec211a4e798f9ba1c12a521d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_67f2b2eb9a004b7b91c7a4181f199b6e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d0c0bb3c052495b865175df8c00ee98",
              "IPY_MODEL_717ad3134739462ba1d275f7a1af82b5"
            ]
          }
        },
        "67f2b2eb9a004b7b91c7a4181f199b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d0c0bb3c052495b865175df8c00ee98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a3aa5be4fcb24dd5a12f21762d79dfb6",
            "_dom_classes": [],
            "description": "epoch 1",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 34555,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 728,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_374d0a21120a4f8fa2fa8f527a054fe0"
          }
        },
        "717ad3134739462ba1d275f7a1af82b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_93ab1900714b4d8ba25033eb12718f73",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  2% 728/34555 [00:40&lt;20:31, 27.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_535bf5dab3f94ba8b181973b2b2f02ea"
          }
        },
        "a3aa5be4fcb24dd5a12f21762d79dfb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "374d0a21120a4f8fa2fa8f527a054fe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93ab1900714b4d8ba25033eb12718f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "535bf5dab3f94ba8b181973b2b2f02ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flying-bear/kompluxternaya/blob/master/assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOhJI37CJnsc",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 7\n",
        "\n",
        "Train a Transformer model for Machine Translation from Russian to English.  \n",
        "Dataset: http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz   \n",
        "Make all source and target text to lower case.  \n",
        "\n",
        "Score: corpus-bleu `nltk.translate.bleu_score.corpus_bleu`  \n",
        "Use last 1000 sentences for model evalutation (test dataset).  \n",
        "Use your target sequence tokenization for BLEU score.  \n",
        "Use max_len=50 for sequence prediction.  \n",
        "\n",
        "\n",
        "Hint: You may consider much smaller model, than shown in the example.  \n",
        "\n",
        "Baselines:  \n",
        "[4 point] BLEU = 0.05  \n",
        "[6 point] BLEU = 0.10  \n",
        "[9 point] BLEU = 0.15  \n",
        "\n",
        "[1 point] Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
        "\n",
        "\n",
        "Readings:\n",
        "1. BLUE score how to https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "1. Transformer code and comments http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-YVev80JuzS",
        "colab_type": "code",
        "outputId": "34fde915-4903-4403-919c-e8dc291b67f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!wget http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-13 19:55:41--  http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113157482 (108M) [application/x-gzip]\n",
            "Saving to: ‘training-parallel-nc-v13.tgz’\n",
            "\n",
            "training-parallel-n 100%[===================>] 107.92M  95.9MB/s    in 1.1s    \n",
            "\n",
            "2020-02-13 19:55:48 (95.9 MB/s) - ‘training-parallel-nc-v13.tgz’ saved [113157482/113157482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5OPsBlKJ4Mq",
        "colab_type": "code",
        "outputId": "748c2a42-0daa-43ef-dd09-02afe78504eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!gunzip -c training-parallel-nc-v13.tgz | tar xvf - "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training-parallel-nc-v13/\n",
            "training-parallel-nc-v13/news-commentary-v13.ru-en.ru\n",
            "training-parallel-nc-v13/news-commentary-v13.cs-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.de-en.de\n",
            "training-parallel-nc-v13/news-commentary-v13.ru-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.zh-en.zh\n",
            "training-parallel-nc-v13/news-commentary-v13.zh-en.en\n",
            "training-parallel-nc-v13/news-commentary-v13.cs-en.cs\n",
            "training-parallel-nc-v13/news-commentary-v13.de-en.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3799109c-eaf6-4325-957c-c5715c96e389",
        "id": "vIToU2TqXqaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!rm training-parallel-nc-v13/news-commentary-v13.cs-en.en\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.de-en.de\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.zh-en.zh\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.zh-en.en\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.cs-en.cs\n",
        "!rm training-parallel-nc-v13/news-commentary-v13.de-en.en\n",
        "\n",
        "!ls training-parallel-nc-v13"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "news-commentary-v13.ru-en.en  news-commentary-v13.ru-en.ru\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQqHFHGaK2wg",
        "colab_type": "code",
        "outputId": "52877e80-0ec7-46d2-eeb9-db32a89ef54f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 9.6MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 9.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSfZTW5vJnsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy \n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import sentencepiece as spm\n",
        "import seaborn\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu as bleu\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Field\n",
        "from torchtext.datasets import TranslationDataset\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHUGNqqmSF8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "max_len=50\n",
        "max_src_in_batch = 25000\n",
        "max_tgt_in_batch = 25000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51VO9rWafLle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else tt.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h48RAm9MIRQ",
        "colab_type": "text"
      },
      "source": [
        "## TOKENIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx9JbkQdO-Vq",
        "colab_type": "text"
      },
      "source": [
        "ENGLISH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6E-1psJMV7H",
        "colab_type": "code",
        "outputId": "13ce444d-0ebf-4671-a7a4-de2ce3a38061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spm.SentencePieceTrainer.Train('--input=training-parallel-nc-v13/news-commentary-v13.ru-en.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HR3MhUlMaF2",
        "colab_type": "code",
        "outputId": "187ffe85-1d74-4452-d01a-d71910146980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qxwL0L3MKnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT = Field(\n",
        "    fix_length=max_len,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqv-rfdPAMm",
        "colab_type": "text"
      },
      "source": [
        "RUSSIAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "99994ca5-a358-4758-f2ec-8571381d38ad",
        "id": "SQREmq5dOy-Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spm.SentencePieceTrainer.Train('--input=training-parallel-nc-v13/news-commentary-v13.ru-en.ru --model_prefix=bpe_ru --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ade8f378-d6bd-4c74-bfcb-50bd4fc5718a",
        "id": "W8EL_AUCOy-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tok_ru = spm.SentencePieceProcessor()\n",
        "tok_ru.load('bpe_ru.model')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2etE_sa4Oy-r",
        "colab": {}
      },
      "source": [
        "TEXT = Field(\n",
        "    fix_length=max_len,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdJPhkqRVQYd",
        "colab_type": "text"
      },
      "source": [
        "## DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfAXuVlaYCrP",
        "colab_type": "text"
      },
      "source": [
        "NB! every sentence is a new line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXMxuf4ZXuuZ",
        "colab_type": "code",
        "outputId": "c9b8c988-1a07-4580-9856-470fa6c0c06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open('training-parallel-nc-v13/news-commentary-v13.ru-en.ru', 'r', encoding='utf-8') as f:\n",
        "  len_source = len(f.readlines())\n",
        "print(len_source)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE6QNoecYCQL",
        "colab_type": "code",
        "outputId": "810c5073-55e0-4215-e805-3164d016f980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open('training-parallel-nc-v13/news-commentary-v13.ru-en.en', 'r', encoding='utf-8') as f:\n",
        "  len_target = len(f.readlines())\n",
        "print(len_target)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfLiUpakYL8R",
        "colab_type": "code",
        "outputId": "e558c93e-d9b7-4312-e238-8e6016fec129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_percent = 1000/len_target \n",
        "test_percent"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.004252441964798286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOOts08wVSe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = TranslationDataset('training-parallel-nc-v13/news-commentary-v13', exts=('.ru-en.ru', '.ru-en.en'), fields=(TEXT, TGT), \n",
        "                             filter_pred=lambda x: len(vars(x)['src']) <= max_len and len(vars(x)['trg']) <= max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O25Mg2vtWSzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn, tst, vld = dataset.split(split_ratio=[0.7, test_percent, 1 - 0.7 - test_percent], stratified=False, random_state=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "roX11K00Oy-w",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(trn.src, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdQ5WgZgMT5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT.build_vocab(trn.trg, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVZsoGTq1lZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = TEXT.vocab.stoi[\"<pad>\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7c_4_xe2HmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_tgt_size = len(TGT.vocab)\n",
        "vocab_src_size = len(TEXT.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO9sqXPgL_5E",
        "colab_type": "text"
      },
      "source": [
        "## MODEL ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eio-sP-pxw2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0ykF_8ryi8o",
        "colab_type": "text"
      },
      "source": [
        "### ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26HrBNXJyGxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMcprSb1yKfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLi814xgyO8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0-xlCzOyQ6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K80bjWp0yVX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCdisr04yd-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL4fviG_ylsi",
        "colab_type": "text"
      },
      "source": [
        "### DECODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS9P7xlUyn3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIrsHMWkyrVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv6M2a12yttx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuXqm-9-yxKh",
        "colab_type": "text"
      },
      "source": [
        "### ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoUmOS5oyy24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RhRrItEy1gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2dJxUidy7LV",
        "colab_type": "text"
      },
      "source": [
        "### POSITION WISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXielEJoy47w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ziV0PZy-8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1viqy56zBHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wUjvefdzGsQ",
        "colab_type": "text"
      },
      "source": [
        "### ALL TOGETHER NOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQB2IvnyzEfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai41imjHzVCu",
        "colab_type": "text"
      },
      "source": [
        "## BATCHES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPDr1PXPzZ3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EUsClDlbQ5_",
        "colab_type": "text"
      },
      "source": [
        "## ITERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H8EMt2hautk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    return Batch(src, trg, pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7L3OuP6eBBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBWaTkVMcm9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_itr = MyIterator(trn, batch_size=BATCH_SIZE, device=device,\n",
        "                     repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                     batch_size_fn=batch_size_fn, train=True)\n",
        "vld_itr = MyIterator(vld, batch_size=BATCH_SIZE, device=device,\n",
        "                     repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                     batch_size_fn=batch_size_fn, train=False)\n",
        "tst_itr = MyIterator(tst, batch_size=BATCH_SIZE, device=device,\n",
        "                     repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                     batch_size_fn=batch_size_fn, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUYs2_fCfdMW",
        "colab_type": "code",
        "outputId": "87457e12-bb80-4c82-bd0c-b524d165a694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "example = next(iter(trn_itr))\n",
        "example"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 2]\n",
              "\t[.src]:[torch.cuda.LongTensor of size 2x50 (GPU 0)]\n",
              "\t[.trg]:[torch.cuda.LongTensor of size 2x50 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tNrNZs3z95G",
        "colab_type": "code",
        "outputId": "d392d6ed-eae7-42a6-c496-fec80c55d493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "example.src"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    2,    60,    62,   229,     5,  2982,    97,    85,  5423,  3982,\n",
              "            41,    66,     4,    12,   223,   903,     7,     7,   646,   461,\n",
              "          8281, 12581,    68,   538,   368,  1027,  1294,  3308,     7, 15458,\n",
              "          2520,     8,   798,    11,  1390,     4,    67,    49,  8512,   326,\n",
              "             6,     3,     1,     1,     1,     1,     1,     1,     1,     1],\n",
              "        [    2,  1331,   105,  9606,   275,   325,     5,    65, 15666,     5,\n",
              "            32, 28457,   262,     5,    25, 23254,  8342, 14883,  1133,     4,\n",
              "            30,  1764,   152,     5, 29996,   108,     4,     0, 19608,  3146,\n",
              "          2997,    63,     7,   550,     4,  7419,  5720,  1027,   204,   101,\n",
              "             6,     3,     1,     1,     1,     1,     1,     1,     1,     1]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rrqsKSNAaeW",
        "colab_type": "text"
      },
      "source": [
        "### TEXT HANDLER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyNnxT1OgmwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_text(seq, field=TEXT):\n",
        "  \"\"\"gets a list of tokens given a list of items using a vocabulary of a given field\n",
        "  \n",
        "  :param seq: list of int, token ids\n",
        "  :param field: trained torchtext.data.Field, optional, default TEXT\n",
        "  :return: list of str, tokens\n",
        "  \"\"\"\n",
        "  return [field.vocab.itos[v] for v in seq]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaOtzBR1g1DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_tok(tok_list, eos_token='</s>', sos_token='</s>', pad_token='<pad>'): \n",
        "  \"\"\"\n",
        "  removes paading and joins subword tokens\n",
        "\n",
        "  :param tok_list: list of str, subword tokens with \"_\" indicating a word beginning (space)\n",
        "  :param eos_token: str, end of sequence token, optional, default </s> \n",
        "  :param sos_token: str, start of sequence token, optional, default <s> \n",
        "  :param pad_token: str, pad token, optional, default <pad> \n",
        "  :return: str, joined sentence\n",
        "  \"\"\"\n",
        "  while tok_list[-1] == pad_token:\n",
        "    tok_list = tok_list[:-1]\n",
        "  if tok_list[-1] == eos_token:\n",
        "    return ''.join(tok_list[:-1]).replace('▁', ' ').strip() + ' ' + eos_token\n",
        "  else:\n",
        "    return ''.join(tok_list).replace('▁', ' ').strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPWm7xcVgAN0",
        "colab_type": "code",
        "outputId": "20636f83-d110-41ce-ba0a-61d7e7875ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "join_tok(to_text(example.src[0], TEXT))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> во время кризиса 1997 года было достигнуто согласие о том, что необходимо изменение в в глобальной финансовой архитектуре: миру нужно добиться больших успехов в предотвращении кризисов и борьбе с ними, когда они наступают. </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_WjGHF42HKy",
        "colab_type": "code",
        "outputId": "01e85543-07ca-4ad8-8d56-bffde7131b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "join_tok(to_text(example.trg[0], TGT))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> in the midst of the 1997 crisis, a consensus developed that there was a need for a change in the global financial architecture: the world needed to do better in preventing crises and dealing with them when they occur. </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjMQN8F07Z-",
        "colab_type": "text"
      },
      "source": [
        "## LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEhin99G1Cju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1265QlK1HSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.reshape(-1, x.size(-1)), y.reshape(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "        return loss.item() * norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiMfZ4pnjYbI",
        "colab_type": "text"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P-_61RvzpQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    counter = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        if batch.ntokens != 0:\n",
        "          out = model.forward(batch.src, batch.trg, \n",
        "                              batch.src_mask, batch.trg_mask)\n",
        "          loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "          total_loss += loss\n",
        "          total_tokens += batch.ntokens\n",
        "          tokens += batch.ntokens\n",
        "          if i % 500 == 1:\n",
        "              elapsed = time.time() - start\n",
        "              print(f\"Epoch Step: {i} Loss: {loss / batch.ntokens} Tokens per Sec: {tokens / elapsed} Time Elapsed {elapsed}\")\n",
        "              start = time.time()\n",
        "              tokens = 0\n",
        "        else:\n",
        "          counter += 1\n",
        "          if counter % 100 == 0:\n",
        "            print('100 zero-size batches')\n",
        "    return total_loss / total_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt2Ebhpu0in5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = make_model(vocab_src_size, vocab_tgt_size, N=2)\n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "criterion.cuda()\n",
        "model_opt = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoP0UJ4N360i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_batches = len(list(iter(trn_itr)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9iTh9iWHkfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rb_trn_itr = (rebatch(pad_idx, b) for b in trn_itr)\n",
        "rb_vld_itr = (rebatch(pad_idx, b) for b in vld_itr)\n",
        "rb_tst_itr = (rebatch(pad_idx, b) for b in tst_itr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yTC2i_JLkXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420,
          "referenced_widgets": [
            "ddb4b7ec211a4e798f9ba1c12a521d8e",
            "67f2b2eb9a004b7b91c7a4181f199b6e",
            "5d0c0bb3c052495b865175df8c00ee98",
            "717ad3134739462ba1d275f7a1af82b5",
            "a3aa5be4fcb24dd5a12f21762d79dfb6",
            "374d0a21120a4f8fa2fa8f527a054fe0",
            "93ab1900714b4d8ba25033eb12718f73",
            "535bf5dab3f94ba8b181973b2b2f02ea"
          ]
        },
        "outputId": "5ecf343d-df49-4566-acf5-fdf8f4c68423"
      },
      "source": [
        "for epoch in range(10):\n",
        "        iterator = tqdm_notebook(rb_trn_itr, total=n_batches, desc=f'epoch {epoch + 1}', leave=True)\n",
        "        model.train()\n",
        "        run_epoch(iterator, model, \n",
        "                  SimpleLossCompute(model.generator, criterion, opt=model_opt))\n",
        "        model.eval()\n",
        "        loss = run_epoch(rb_vld_itr, model, \n",
        "                          SimpleLossCompute(model.generator, criterion, None))\n",
        "        print(loss)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddb4b7ec211a4e798f9ba1c12a521d8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='epoch 1', max=34555, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 1 Loss: 0.13947680592536926 Tokens per Sec: 220.42523193359375 Time Elapsed 0.5126454830169678\n",
            "Epoch Step: 501 Loss: 0.15192009508609772 Tokens per Sec: 2037.09423828125 Time Elapsed 18.076728105545044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-91308a4e5ced>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         run_epoch(iterator, model, \n\u001b[0;32m----> 5\u001b[0;31m                   SimpleLossCompute(model.generator, criterion, opt=model_opt))\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         loss = run_epoch(rb_vld_itr, model, \n",
            "\u001b[0;32m<ipython-input-60-590b986a14c5>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           out = model.forward(batch.src, batch.trg, \n\u001b[0;32m---> 11\u001b[0;31m                               batch.src_mask, batch.trg_mask)\n\u001b[0m\u001b[1;32m     12\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8b5c5d485387>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m\"Take in and process masked src and target sequences.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         return self.decode(self.encode(src, src_mask), src_mask,\n\u001b[0;32m---> 17\u001b[0;31m                             tgt, tgt_mask)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8b5c5d485387>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, memory, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-4cfe73c881ce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-4d66f66320a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b1fea0696ed6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"Apply residual connection to any sublayer with the same size.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-cb52f652a350>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_3MJXROK5zi",
        "colab_type": "text"
      },
      "source": [
        "## DECODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOOqDLIkK82S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys\n",
        "\n",
        "model.eval()\n",
        "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
        "src_mask = Variable(torch.ones(1, 1, 10) )\n",
        "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSOx2etMMPwW",
        "colab_type": "text"
      },
      "source": [
        "ADAPT FROM LAST HW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdTjgIgDMN4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search_from_start(model, k=3, max_len=30, eos='<eos>'):\n",
        "  \"\"\"\n",
        "  autoregressive language modeling beam search decoder\n",
        "\n",
        "  :param model: trained torch model, predicts logits torch tensor (of shape (batch, vocab, seq)) form batch tensor (of shape (batch, seq))\n",
        "  :param k: int, beam width, optional, default 3\n",
        "  :param max_len: int, maximal number of words produced, optional, default 30\n",
        "  :param eos: str, end of sentence token, optional, default '<eos>'\n",
        "  :return: list of str (of len k), top predicted sequences\n",
        "  \"\"\"\n",
        "  id_eos = token_to_id(eos)\n",
        "  with tt.no_grad():\n",
        "    pred = model(tt.tensor([[start]]))[:,:,-1][0]\n",
        "    toks = tt.argsort(pred, descending=True)[:k].tolist()\n",
        "    tok_probs = pred[toks].tolist()\n",
        "    candidates = [([start, toks[i]], tok_probs[i]) for i in range(len(toks))]\n",
        "    while max_len > max([len(seq) for seq, prob in candidates]):\n",
        "      # expand each current candidate that doesn't have an EOS at the end\n",
        "      new_candidates = []\n",
        "      for seq, prob in candidates:\n",
        "        if seq[-1] != id_eos:\n",
        "          pred = model(tt.tensor([seq]))[:,:,-1][0]\n",
        "          toks = tt.argsort(pred, descending=True)[:k].tolist()\n",
        "          # probs = np.log(soft_with_temp(pred))\n",
        "          tok_probs = pred[toks].tolist()\n",
        "          new_candidates += [(seq[:]+[toks[i]], prob+tok_probs[i]) for i in range(len(toks))]\n",
        "        else:\n",
        "          new_candidates += [(seq, prob)]\n",
        "      candidates = sorted(new_candidates, key=lambda tup:tup[1])[:k] # order all candidates by score and select k best\n",
        "  return [join_tok(to_text(seq)) for seq, prob in candidates]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}