{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flying-bear/kompluxternaya/blob/master/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSsqcR7Tb9DB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Assignment 1\n",
        "\n",
        "\n",
        "Using texts http://www.gutenberg.org/files/2600/2600-0.txt\n",
        "1. Make texts lowercase and remove all punctuation except spaces and dots.\n",
        "2. Tokenize texts by BPE with vocab_size = 100\n",
        "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
        "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
        "5. Calculate perplexity of the language model for the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1v5OMqAKhAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from sklearn.base import TransformerMixin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn5f4JDpb9DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "286c0cd4-85b4-4413-82f7-4391a9036f87"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vvv269v5pUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = 'gdrive/My Drive/studies/HSE/prog/kompluxternaya'\n",
        "with open(root_path+'/'+'peace.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Fi2UAjb9Dc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b5ec4dd-afc6-4126-acba-1f4787245261"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # TODO\n",
        "    # make lowercase\n",
        "    text = text.lower()\n",
        "    # replace all punctuation except dots with spaces\n",
        "    pattern = '!|\"|#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|^|_|`|{|\\||}|~|”|“|—|‘|’'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    # collapse multiple spaces into one '   ' -> ' '\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "text = preprocess_text(text)\n",
        "#assert len(text) == 3141169\n",
        "len(text)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3141171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_YETckkb9Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text.split('.')\n",
        "text = [x.strip() for x in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeRdojT9b9Do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_bigrams(l):\n",
        "  \"\"\"\n",
        "  takes a list of lists of integers or strings\n",
        "  returns a counter of their pairs (tuples) across the lists\n",
        "  \"\"\"\n",
        "  bigrams = Counter()\n",
        "  for i in range(len(l)):\n",
        "    bigrams.update((x, y) for x, y in zip(*[l[i][j:] for j in range(2)]))\n",
        "  return bigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrKNPKbSZKbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_token(text, token, token_id):\n",
        "  \"\"\"\n",
        "  takes \n",
        "  - a text (list of lists of integers)\n",
        "  - a token (integer tuple)\n",
        "  - the id of the given token (integer)\n",
        "  returns a modified text, where all instances of the token (element pairs) are replaced by token_id\n",
        "  \"\"\"\n",
        "  text_new = text.copy()\n",
        "  for i, sent in enumerate(text):\n",
        "    for j, (v, w) in enumerate(zip(sent[:-1], sent[1:])):\n",
        "      if (v, w) == token:\n",
        "        text_new[i][j] = token_id\n",
        "        del text_new[i][j-1]\n",
        "  return text_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7xGXMYyrtyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8878955e-4c6d-4e2d-9b45-7a5c5a51554b"
      },
      "source": [
        "t = ['рофапуа оыврк !! 11', 'рырары 22']\n",
        "print([len(i) for i in t])\n",
        "itos = list(set(''.join(t))) # list letters\n",
        "stoi = dict(zip(itos, range(len(itos)))) # dict of letters and their ids\n",
        "t = [[stoi[x] for x in t[i]] for i in range(len(t))] # replace letters with thir ids (str to list)\n",
        "vocab_size = 20\n",
        "while len(itos) < vocab_size:\n",
        "    bigrams = list_to_bigrams(t) # get text bigrams\n",
        "    if bigrams.most_common(1):\n",
        "      new_token = bigrams.most_common(1)[0][0] # find most common bigram\n",
        "    else:\n",
        "      break\n",
        "    new_id = len(itos)\n",
        "    itos.append(new_token)\n",
        "    stoi[new_token] = new_id\n",
        "    # find occurences of the new_token in the text and replace them with new_id\n",
        "    t = update_token(t, new_token, new_id) \n",
        "print([len(i) for i in t])\n",
        "print(itos)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19, 9]\n",
            "[15, 3]\n",
            "['р', '2', 'ы', 'в', 'а', '1', '!', ' ', 'к', 'о', 'ф', 'п', 'у', (0, 2), (13, 2), (14, 2), (0, 9), (16, 9), (17, 9), (18, 9)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUbIJjBAWGqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d317f63-59e4-4e5c-b60d-addd8fcd707f"
      },
      "source": [
        "itos = ['а', 'е', 'и', 'р', (1,2), (0,3), (4,0), (4,5)]\n",
        "def recursive_token_lookup(tok): #tok int or tuple\n",
        "  if type(tok) == int:\n",
        "    content = itos[tok]\n",
        "    if type(content) == str:\n",
        "      return content\n",
        "    else:\n",
        "      return recursive_token_lookup(content)\n",
        "  elif type(tok) == tuple:\n",
        "    return recursive_token_lookup(tok[0]) + recursive_token_lookup(tok[1])\n",
        "  \n",
        "recursive_token_lookup(6)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'еиа'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1CNokZ4ub9Dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "8978a4f1-5a38-4319-c510-7a14d15e1d73"
      },
      "source": [
        "class BPE(TransformerMixin):\n",
        "    def __init__(self, vocab_size=100):\n",
        "        super(BPE, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # index to token\n",
        "        self.itos = []\n",
        "        # token to index\n",
        "        self.stoi = {}\n",
        "\n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        takes a text (list of strings)\n",
        "        fits \n",
        "        - self.itos (a list of strings (symbols) and integer tuples (most frequent bigrams while vocabulary does not exceed vocab_size\n",
        "                      each symbol/bigram is indexed by its place in the list) )\n",
        "        - self.stoi (a dict of symbols and bigrams to thir indicies in the self.stoi list)\n",
        "        - text (symbols replaced by thir ids, bigrams of ids replaced by their ids)\n",
        "        returns self\n",
        "        \"\"\"\n",
        "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
        "        self.itos = list(set(''.join(text))) # list letters\n",
        "        self.stoi = dict(zip(self.itos, range(len(self.itos)))) # dict of letters and their ids\n",
        "        text = [[self.stoi[x] for x in text[i]] for i in range(len(text))] # replace letters with thir ids (str to list)\n",
        "        \n",
        "        while len(self.itos) < self.vocab_size:\n",
        "            bigrams = list_to_bigrams(text) # get text bigrams\n",
        "            if bigrams.most_common(1):\n",
        "              new_token = bigrams.most_common(1)[0][0] # find most common bigram\n",
        "            else:\n",
        "              break\n",
        "            new_id = len(self.itos)\n",
        "            self.itos.append(new_token)\n",
        "            self.stoi[new_token] = new_id\n",
        "            # find occurences of the new_token in the text and replace them with new_id\n",
        "            text = update_token(text, new_token, new_id)  \n",
        "        return self\n",
        "    \n",
        "    def transform(self, text):\n",
        "        \"\"\"\n",
        "        takes a text (list of strings)\n",
        "        convert text to a sequence of symbol ids then replaces bigrams of ids with their indicies in self.stoi\n",
        "        returns modified text\n",
        "        \"\"\"\n",
        "        text_in_vocabulary = [[symbol for symbol in sent if symbol in self.itos] for sent in text] # exclude out of vocabulary symbols\n",
        "        text = [[self.stoi[letter] for letter in sent] for sent in text_in_vocabulary] # tokenize text by symbols using self.stoi\n",
        "        for token_id, token in enumerate(self.itos): # find occurences of a complex token in the text and replace them with token_id\n",
        "            text = update_token(text, token, token_id)    \n",
        "        return text\n",
        "    \n",
        "    def decode_token(self, tok):\n",
        "        \"\"\"\n",
        "        takes a tok (either an int - id, or a tuple - pair of ids)\n",
        "        returns a text coded by the tok\n",
        "        \"\"\"\n",
        "        def recursive_token_lookup(token): #token int or tuple\n",
        "          if type(token) == int:\n",
        "            content = itos[token]\n",
        "            if type(content) == str:\n",
        "              return content # only returns strings\n",
        "            else:\n",
        "              return recursive_token_lookup(content) # continue lookup on the tuple that was found\n",
        "          elif type(token) == tuple:\n",
        "            return recursive_token_lookup(token[0]) + recursive_token_lookup(token[1]) # concatenate string results\n",
        "        return recursive_token_lookup(tok)\n",
        "            \n",
        "    def decode(self, text):\n",
        "        \"\"\"\n",
        "        convert token ids into text\n",
        "        \"\"\"\n",
        "        return ''.join(map(self.decode_token, text))\n",
        "        \n",
        "        \n",
        "vocab_size = 100\n",
        "bpe = BPE(vocab_size)\n",
        "tokenized_text = bpe.fit_transform(text)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-7ff5ea290bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mbpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-7ff5ea290bc6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_token\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# find occurences of the new_token in the text and replace them with new_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-32b88e3273e0>\u001b[0m in \u001b[0;36mupdate_token\u001b[0;34m(text, token, token_id)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtext_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtext_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtext_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx7iTb4tpaZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d450d5fe-ba78-41e4-c25c-a5aa4981e9d0"
      },
      "source": [
        "tokenized_text"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[30], [37]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DTU3BJQb9D5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "1cc8ecd6-836f-4e2a-dfd3-ee9c0cfca055"
      },
      "source": [
        "assert bpe.decode(tokenized_text[0]) == texts[0]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-225d8424af76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mbpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-7ff5ea290bc6>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mconvert\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mids\u001b[0m \u001b[0minto\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"\"\"\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-7ff5ea290bc6>\u001b[0m in \u001b[0;36mdecode_token\u001b[0;34m(self, tok)\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecursive_token_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecursive_token_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# concatenate string results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrecursive_token_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-7ff5ea290bc6>\u001b[0m in \u001b[0;36mrecursive_token_lookup\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrecursive_token_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#token int or tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m               \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;31m# only returns strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1MWgL6sb9EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_token = vocab_size\n",
        "end_token = vocab_size + 1\n",
        "        \n",
        "    \n",
        "class LM:\n",
        "    def __init__(self, vocab_size, delta=1):\n",
        "        self.delta = delta\n",
        "        self.vocab_size = vocab_size + 2\n",
        "        self.proba = # TODO create array for storing 3-gram counters\n",
        "        \n",
        "    def infer(self, a, b, tau=1):\n",
        "        \"\"\"\n",
        "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        result = # TODO\n",
        "        return result\n",
        "        \n",
        "    def get_proba(self, a, b, c, tau=1):\n",
        "        \"\"\"\n",
        "        get probability of 3-gram (a,b,c)\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        c: third token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        result = # TODO approximate probability by counters\n",
        "        return result\n",
        "    \n",
        "    def fit(self, texts):\n",
        "        \"\"\"\n",
        "        train language model on texts\n",
        "        texts: list of lists\n",
        "        \"\"\"\n",
        "        self.proba = # TODO count 3-grams in the texts\n",
        "        \n",
        "        return self\n",
        "    \n",
        "lm = LM(vocab_size, 1).fit(tokenized_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek7jeggLb9ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
        "    \"\"\"\n",
        "    generate sequence from language model *lm* conditioned on input_seq\n",
        "    input_seq: sequence of token ids for conditioning\n",
        "    lm: language model\n",
        "    max_len: max generated sequence length\n",
        "    k: size of beam\n",
        "    tau: temperature\n",
        "    \"\"\"\n",
        "    \n",
        "    beam = # TODO store in beam tuples of current sequences and their log probabilities\n",
        "    \n",
        "    for i in range(max_len):\n",
        "        candidates = []\n",
        "        candidates_proba = []\n",
        "        for snt, snt_proba in beam:\n",
        "            if # TODO process terminal token\n",
        "            else:    \n",
        "                proba = # probability vector of the next token\n",
        "                best_k = # top-k most probable tokens\n",
        "                # TODO update candidates' sequences and corresponding probabilities\n",
        "                \n",
        "        beam = # select top-k most probable sequences from candidates\n",
        "    return beam\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXXNH0ieb9EZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = 'horse '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxqKvnvAb9Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = 'her'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA7l1zyNb9Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = 'what'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
        "# TODO print decoded generated strings and their probabilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZSZy_cCb9Eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = 'gun '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHhRyjYfb9E3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(snt, lm):\n",
        "    \"\"\"\n",
        "    snt: sequence of token ids\n",
        "    lm: language model\n",
        "    \"\"\"\n",
        "    result = #TODO perplexity for the sentence\n",
        "    return result\n",
        "\n",
        "perplexity(tokenized_texts[0], lm)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}